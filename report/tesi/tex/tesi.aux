\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{toc}{\contentsline {chapter}{Indice}{i}{Doc-Start}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Apprendimento automatico}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap:apprendimento_automatico}{{1}{1}{Apprendimento automatico}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Approcci}{1}{section.1.1}}
\newlabel{sec:approcci}{{1.1}{1}{Approcci}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Supervisionato}{1}{section.1.2}}
\citation{intro_machine_learning}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Statistica per determinare l'acquisto di una nuova barca}}{3}{table.1.1}}
\newlabel{table_customers}{{1}{3}{Statistica per determinare l'acquisto di una nuova barca}{table.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Trade-off tra overfitting e underfitting}}{4}{figure.1.1}}
\newlabel{fig:tradeoff_img}{{1}{4}{Trade-off tra overfitting e underfitting}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}k-Nearest Neighbor}{4}{subsection.1.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Classificazione tramite k-NN di 3 elementi. Le stelle rosse rappresentano una classificazione rispetto alla classe 1 (dei triangoli) mentre le stelle blu rappresentano una classificazione rispetto alla classe 0 (dei cerchi).}}{5}{figure.1.2}}
\newlabel{fig:knn_classifier}{{2}{5}{Classificazione tramite k-NN di 3 elementi. Le stelle rosse rappresentano una classificazione rispetto alla classe 1 (dei triangoli) mentre le stelle blu rappresentano una classificazione rispetto alla classe 0 (dei cerchi)}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Risultati di un classificatore k-NN, per diversi valori del parametro $k$ su un medesimo dataset. I cerchi e i triangoli indicano le osservazioni del dataset appartenenti a due classi, mentre le aree blu e rosse indicano gli esiti della classificazione.}}{5}{figure.1.3}}
\newlabel{fig:knn_difference}{{3}{5}{Risultati di un classificatore k-NN, per diversi valori del parametro $k$ su un medesimo dataset. I cerchi e i triangoli indicano le osservazioni del dataset appartenenti a due classi, mentre le aree blu e rosse indicano gli esiti della classificazione}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Risultato di un regressore k-NN, per $k$ pari a 3. I cerchi indicano le osservazioni del dataset, le stelle verdi indicano il nuovo elemento da predire e le stelle blu gli esiti della predizione.}}{6}{figure.1.4}}
\newlabel{fig:knn_regressor}{{4}{6}{Risultato di un regressore k-NN, per $k$ pari a 3. I cerchi indicano le osservazioni del dataset, le stelle verdi indicano il nuovo elemento da predire e le stelle blu gli esiti della predizione}{figure.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Modelli lineari}{6}{subsection.1.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Regressione lineare. I punti rappresentano i pazienti e la retta nera rappresenta la retta di regressione che approssima meglio all'andamento di tutti i punti}}{7}{figure.1.5}}
\newlabel{fig:linear_regression}{{5}{7}{Regressione lineare. I punti rappresentano i pazienti e la retta nera rappresenta la retta di regressione che approssima meglio all'andamento di tutti i punti}{figure.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Support Vector Machine}{7}{subsection.1.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Possibili iperpiani che dividono lo spazio (a sinistra) e iperpiano ``migliore" (a destra)}}{8}{figure.1.6}}
\newlabel{fig:svc}{{6}{8}{Possibili iperpiani che dividono lo spazio (a sinistra) e iperpiano ``migliore" (a destra)}{figure.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Dati non divisibili linearmente}}{9}{figure.1.7}}
\newlabel{fig:svc_non_linear}{{7}{9}{Dati non divisibili linearmente}{figure.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Dati visti sull'asse z}}{10}{figure.1.8}}
\newlabel{fig:svc_on_z_axis}{{8}{10}{Dati visti sull'asse z}{figure.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Alberi di decisione}{10}{subsection.1.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Esempio di albero di decisione per la classificazione di un animale}}{10}{figure.1.9}}
\newlabel{fig:example_decision_tree}{{9}{10}{Esempio di albero di decisione per la classificazione di un animale}{figure.1.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Overfitting dei dati con l'albero di decisione}}{11}{figure.1.10}}
\newlabel{fig:overfit_decision_trees1}{{10}{11}{Overfitting dei dati con l'albero di decisione}{figure.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Grafico di \emph  {overfitting} per diversi alberi di decisione in cui ogni punto corrisponde ad un modello DT la cui profondit\IeC {\`a} \IeC {\`e} definita sull'asse delle ascisse. La curva nera rappresenta l'errore di precisione nella predizione di dati di allenamento mentre la curva rossa rappresenta l'errore sui dati di test. }}{12}{figure.1.11}}
\newlabel{fig:overfit_decision_trees2}{{11}{12}{Grafico di \emph {overfitting} per diversi alberi di decisione in cui ogni punto corrisponde ad un modello DT la cui profondità è definita sull'asse delle ascisse. La curva nera rappresenta l'errore di precisione nella predizione di dati di allenamento mentre la curva rossa rappresenta l'errore sui dati di test}{figure.1.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Non supervisionato}{13}{section.1.3}}
\citation{ruder}
\citation{ruder}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Semi-supervisionato}{14}{section.1.4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces self-training}}{15}{algorithm.1}}
\newlabel{algo:self_training}{{1}{15}{Semi-supervisionato}{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Apprendimento con rinforzo}{15}{section.1.5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Riduzione della dimensionalit\IeC {\`a}}{15}{section.1.6}}
\newlabel{fig:rl_scenario}{{1.5}{16}{Apprendimento con rinforzo}{section.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Scenario RL: l'Agent compie un'azione in un Environemnt che porta a un cambiamento di stato e a una ricompensa che influenzeranno le future scelte dell'Agent}}{16}{figure.1.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Analisi delle componenti principali}{16}{subsection.1.6.1}}
\citation{3d_face_recocgnition_pca}
\citation{image_compression_pca}
\citation{pattern_recocgnition_pca}
\citation{data_mining_pca}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}t-Distributed stochastic neighbor embedding}{17}{subsection.1.6.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Induzione di insiemi fuzzy}{19}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:prova}{{2}{19}{Induzione di insiemi fuzzy}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}La logica fuzzy}{19}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Gli insiemi fuzzy}{19}{section.2.2}}
\citation{fuzzylearn}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Fuzzylearn}{20}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Calcolo dell'insieme fuzzy}{21}{subsection.2.3.1}}
\citation{fletcher_optimization}
\newlabel{eq:1}{{1}{22}{Calcolo dell'insieme fuzzy}{equation.2.3.1}{}}
\newlabel{eq:2}{{2}{22}{Calcolo dell'insieme fuzzy}{equation.2.3.2}{}}
\newlabel{eq:3}{{3}{22}{Calcolo dell'insieme fuzzy}{equation.2.3.3}{}}
\citation{ben_hur}
\citation{fuzzylearn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Configurazione degli iperparametri}{24}{subsection.2.3.2}}
\citation{ben_hur}
\newlabel{fig:lin}{{13(a)}{25}{Subfigure 2 13(a)}{subfigure.13.1}{}}
\newlabel{sub@fig:lin}{{(a)}{25}{Subfigure 2 13(a)\relax }{subfigure.13.1}{}}
\newlabel{fig:qlin}{{13(b)}{25}{Subfigure 2 13(b)}{subfigure.13.2}{}}
\newlabel{sub@fig:qlin}{{(b)}{25}{Subfigure 2 13(b)\relax }{subfigure.13.2}{}}
\newlabel{fig:exp}{{13(c)}{25}{Subfigure 2 13(c)}{subfigure.13.3}{}}
\newlabel{sub@fig:exp}{{(c)}{25}{Subfigure 2 13(c)\relax }{subfigure.13.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Esempio di \emph  {fuzzifiers} per un insieme I. L'asse delle ordinate rappresenta il grado di appartenenza e l'asse delle ascisse rappresenta la distanza dalla sfera. Al crescere della distanza diminuisce il grado di appartenenza all'insieme I secondo il \emph  {fuzzifier}.}}{25}{figure.2.13}}
\newlabel{fig:fuzzifier}{{13}{25}{Esempio di \emph {fuzzifiers} per un insieme I. L'asse delle ordinate rappresenta il grado di appartenenza e l'asse delle ascisse rappresenta la distanza dalla sfera. Al crescere della distanza diminuisce il grado di appartenenza all'insieme I secondo il \emph {fuzzifier}}{figure.2.13}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\mathaccentV{hat}05E\mu _{\text {lin}}$}}}{25}{figure.2.13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\mathaccentV{hat}05E\mu _{\text {qlin}}$}}}{25}{figure.2.13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\mathaccentV{hat}05E\mu _{\text {exp}}$}}}{25}{figure.2.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Nelle figure \IeC {\`e} mostrato come incrementando $C$ aumenti la sua ``rigidit\IeC {\`a}" aumenta la sua larghezza assomigliando sempre pi\IeC {\`u} ad una ``funzione di appartenenza" di un insieme standard.}}{26}{figure.2.14}}
\newlabel{fig:possibilearn_c}{{14}{26}{Nelle figure è mostrato come incrementando $C$ aumenti la sua ``rigidità" aumenta la sua larghezza assomigliando sempre più ad una ``funzione di appartenenza" di un insieme standard}{figure.2.14}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$C = 0.0421$}}}{26}{figure.2.14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$C = 0.08$}}}{26}{figure.2.14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$C = 0.15$}}}{26}{figure.2.14}}
\newlabel{fig:first_sub}{{15(a)}{26}{Subfigure 2 15(a)}{subfigure.15.1}{}}
\newlabel{sub@fig:first_sub}{{(a)}{26}{Subfigure 2 15(a)\relax }{subfigure.15.1}{}}
\newlabel{fig:second_sub}{{15(b)}{26}{Subfigure 2 15(b)}{subfigure.15.2}{}}
\newlabel{sub@fig:second_sub}{{(b)}{26}{Subfigure 2 15(b)\relax }{subfigure.15.2}{}}
\newlabel{fig:third_sub}{{15(c)}{26}{Subfigure 2 15(c)}{subfigure.15.3}{}}
\newlabel{sub@fig:third_sub}{{(c)}{26}{Subfigure 2 15(c)\relax }{subfigure.15.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Nelle figure \IeC {\`e} mostrato come incrementando $\sigma $ si modifica la forma della funzione di appartenenza.}}{26}{figure.2.15}}
\newlabel{fig:possibilearn_sigma}{{15}{26}{Nelle figure è mostrato come incrementando $\sigma $ si modifica la forma della funzione di appartenenza}{figure.2.15}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\sigma = 0.075$}}}{26}{figure.2.15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\sigma = 0.12$}}}{26}{figure.2.15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\sigma = 2$}}}{26}{figure.2.15}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Esperimenti}{27}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}I dataset}{27}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Olivetti dataset}{27}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}AT\&T faces dataset}{27}{subsection.3.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Valutazione del modello}{28}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Training, Validation e Test set}{28}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Cross-validation}{28}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Grid Search}{29}{subsection.3.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Grid Search con Cross-Validation e Nested Grid Search}{30}{subsection.3.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}I risultati}{30}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Utilizzo metodo di riduzione t-SNE}{32}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Utilizzo metodo di riduzione PCA}{33}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Analisi conclusiva}{34}{subsection.3.3.3}}
\bibcite{intro_machine_learning}{1}
\bibcite{3d_face_recocgnition_pca}{2}
\bibcite{image_compression_pca}{3}
\bibcite{pattern_recocgnition_pca}{4}
\bibcite{data_mining_pca}{5}
\bibcite{fuzzylearn}{6}
\bibcite{fuzzylearn_charts}{7}
\bibcite{towards_data_science}{8}
\bibcite{ben_hur}{9}
\bibcite{fletcher_optimization}{10}
\bibcite{python_data_science}{11}
\bibcite{ruder}{12}
\bibcite{knuthwebsite}{13}
