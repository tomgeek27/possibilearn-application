\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{toc}{\contentsline {chapter}{Indice}{i}{Doc-Start}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Apprendimento automatico}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap:apprendimento_automatico}{{1}{1}{Apprendimento automatico}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Approcci}{1}{section.1.1}}
\newlabel{sec:approcci}{{1.1}{1}{Approcci}{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Supervisionato}{1}{subsection.1.1.1}}
\citation{intro_machine_learning}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Statistica per determinare l'acquisto di una nuova barca}}{3}{table.1.1}}
\newlabel{table_customers}{{1}{3}{Statistica per determinare l'acquisto di una nuova barca}{table.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Trade-off tra overfitting e underfitting}}{4}{figure.1.1}}
\newlabel{fig:tradeoff_img}{{1}{4}{Trade-off tra overfitting e underfitting}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.1}k-Nearest Neighbor}{4}{subsubsection.1.1.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Classificazione tramite k-NN di 3 elementi. Le stelle rosse rappresentano una classificazione rispetto alla classe 1 (dei triangoli) mentre le stelle blu rappresentano una classificazione rispetto alla classe 0 (dei cerchi).}}{5}{figure.1.2}}
\newlabel{fig:knn_classifier}{{2}{5}{Classificazione tramite k-NN di 3 elementi. Le stelle rosse rappresentano una classificazione rispetto alla classe 1 (dei triangoli) mentre le stelle blu rappresentano una classificazione rispetto alla classe 0 (dei cerchi)}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Risultati di un classificatore k-NN, per diversi valori del parametro $k$ su un medesimo dataset. I cerchi e i triangoli indicano le osservazioni del dataset appartenenti a due classi, mentre le aree blu e rosse indicano gli esiti della classificazione.}}{5}{figure.1.3}}
\newlabel{fig:knn_difference}{{3}{5}{Risultati di un classificatore k-NN, per diversi valori del parametro $k$ su un medesimo dataset. I cerchi e i triangoli indicano le osservazioni del dataset appartenenti a due classi, mentre le aree blu e rosse indicano gli esiti della classificazione}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Risultato di un regressore k-NN, per $k$ pari a 3. I cerchi indicano le osservazioni del dataset, le stelle verdi indicano il nuovo elemento da predire e le stelle blu gli esiti della predizione.}}{6}{figure.1.4}}
\newlabel{fig:knn_regressor}{{4}{6}{Risultato di un regressore k-NN, per $k$ pari a 3. I cerchi indicano le osservazioni del dataset, le stelle verdi indicano il nuovo elemento da predire e le stelle blu gli esiti della predizione}{figure.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.2}Modelli lineari}{6}{subsubsection.1.1.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Regressione lineare. I punti rappresentano i pazienti e la retta nera rappresenta la retta di regressione che approssima meglio all'andamento di tutti i punti}}{7}{figure.1.5}}
\newlabel{fig:linear_regression}{{5}{7}{Regressione lineare. I punti rappresentano i pazienti e la retta nera rappresenta la retta di regressione che approssima meglio all'andamento di tutti i punti}{figure.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.3}Support Vector Machine}{7}{subsubsection.1.1.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Possibili iperpiani che dividono lo spazio (a sinistra) e iperpiano ``migliore" (a destra)}}{8}{figure.1.6}}
\newlabel{fig:svc}{{6}{8}{Possibili iperpiani che dividono lo spazio (a sinistra) e iperpiano ``migliore" (a destra)}{figure.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Dati non divisibili linearmente}}{9}{figure.1.7}}
\newlabel{fig:svc_non_linear}{{7}{9}{Dati non divisibili linearmente}{figure.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Dati visti sull'asse z}}{10}{figure.1.8}}
\newlabel{fig:svc_on_z_axis}{{8}{10}{Dati visti sull'asse z}{figure.1.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.4}Alberi di decisione}{10}{subsubsection.1.1.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Esempio di albero di decisione per la classificazione di un animale}}{10}{figure.1.9}}
\newlabel{fig:example_decision_tree}{{9}{10}{Esempio di albero di decisione per la classificazione di un animale}{figure.1.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Overfitting dei dati con l'albero di decisione}}{11}{figure.1.10}}
\newlabel{fig:overfit_decision_trees1}{{10}{11}{Overfitting dei dati con l'albero di decisione}{figure.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Grafico di overfitting con l'albero di decisione. La curva nera rappresenta l'errore di precisione nella predizione di dati di allenamento mentre la curva rossa rappresenta l'errore sui dati di test}}{12}{figure.1.11}}
\newlabel{fig:overfit_decision_trees2}{{11}{12}{Grafico di overfitting con l'albero di decisione. La curva nera rappresenta l'errore di precisione nella predizione di dati di allenamento mentre la curva rossa rappresenta l'errore sui dati di test}{figure.1.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Non supervisionato}{13}{subsection.1.1.2}}
\citation{ruder}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Semi-supervisionato}{14}{subsection.1.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Apprendimento con rinforzo}{15}{subsection.1.1.4}}
\newlabel{fig:rl_scenario}{{1.1.4}{15}{Apprendimento con rinforzo}{subsection.1.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Scenario RL}}{15}{figure.1.12}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Ridimensionamento delle funzionalit\IeC {\`a}}{16}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Principal Component Analysis}{16}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}t-Distributed stochastic neighbor embedding}{17}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Induzione di insiemi fuzzy}{19}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:prova}{{2}{19}{Induzione di insiemi fuzzy}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}La logica fuzzy}{19}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Gli insiemi fuzzy}{19}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Possibilearn}{20}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Calcolo dell'insieme fuzzy}{21}{subsection.2.3.1}}
\newlabel{eq:1}{{1}{21}{Calcolo dell'insieme fuzzy}{equation.2.3.1}{}}
\newlabel{eq:2}{{2}{21}{Calcolo dell'insieme fuzzy}{equation.2.3.2}{}}
\newlabel{eq:3}{{3}{21}{Calcolo dell'insieme fuzzy}{equation.2.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Configurazione degli iperparametri}{23}{subsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Esempio di Linear fuzzifier ($\mathaccentV {hat}05E\mu _{\text  {lin}}$) per un insieme I. L'asse delle ordinate rappresenta il grado di appartenenza e l'asse delle ascisse rappresenta il valore della \emph  {feature}. Al crescere della \emph  {feature} diminuisce il grado di appartenenza all'insieme I in modo lineare.}}{24}{figure.2.13}}
\newlabel{fig:fuzzifier}{{13}{24}{Esempio di Linear fuzzifier ($\hat \mu _{\text {lin}}$) per un insieme I. L'asse delle ordinate rappresenta il grado di appartenenza e l'asse delle ascisse rappresenta il valore della \emph {feature}. Al crescere della \emph {feature} diminuisce il grado di appartenenza all'insieme I in modo lineare}{figure.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Nelle figure \IeC {\`e} mostrato come incrementando $C$ aumenti la sua ``rigidit\IeC {\`a}" aumenta la sua larghezza assomigliando sempre pi\IeC {\`u} ad una ``funzione di appartenenza" di un insieme standard}}{24}{figure.2.14}}
\newlabel{fig:possibilearn_c}{{14}{24}{Nelle figure è mostrato come incrementando $C$ aumenti la sua ``rigidità" aumenta la sua larghezza assomigliando sempre più ad una ``funzione di appartenenza" di un insieme standard}{figure.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Nelle figure \IeC {\`e} mostrato come incrementando $\sigma $ si modifica la forma della funzione di appartenenza}}{25}{figure.2.15}}
\newlabel{fig:possibilearn_sigma}{{15}{25}{Nelle figure è mostrato come incrementando $\sigma $ si modifica la forma della funzione di appartenenza}{figure.2.15}{}}
\bibcite{intro_machine_learning}{1}
\bibcite{towards_data_science}{2}
\bibcite{python_data_science}{3}
\bibcite{ruder}{4}
\bibcite{knuthwebsite}{5}
