\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{toc}{\contentsline {chapter}{Indice}{i}{Doc-Start}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Apprendimento automatico}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap:apprendimento_automatico}{{1}{1}{Apprendimento automatico}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Approcci}{1}{section.1.1}}
\newlabel{sec:approcci}{{1.1}{1}{Approcci}{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Supervisionato}{1}{subsection.1.1.1}}
\citation{intro_machine_learning}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Statistica per determinare l'acquisto di una nuova barca}}{3}{table.1.1}}
\newlabel{table_customers}{{1}{3}{Statistica per determinare l'acquisto di una nuova barca}{table.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Trade-off tra overfitting e underfitting}}{4}{figure.1.1}}
\newlabel{fig:tradeoff_img}{{1}{4}{Trade-off tra overfitting e underfitting}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.1}k-Nearest Neighbor}{4}{subsubsection.1.1.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Classificazione tramite k-NN di 3 elementi. Le stelle rosse rappresentano una classificazione rispetto alla classe 1 (dei triangoli) mentre le stelle blu rappresentano una classificazione rispetto alla classe 0 (dei cerchi).}}{5}{figure.1.2}}
\newlabel{fig:knn_classifier}{{2}{5}{Classificazione tramite k-NN di 3 elementi. Le stelle rosse rappresentano una classificazione rispetto alla classe 1 (dei triangoli) mentre le stelle blu rappresentano una classificazione rispetto alla classe 0 (dei cerchi)}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Risultati di un classificatore k-NN, per diversi valori del parametro $k$ su un medesimo dataset. I cerchi e i triangoli indicano le osservazioni del dataset appartenenti a due classi, mentre le aree blu e rosse indicano gli esiti della classificazione.}}{5}{figure.1.3}}
\newlabel{fig:knn_difference}{{3}{5}{Risultati di un classificatore k-NN, per diversi valori del parametro $k$ su un medesimo dataset. I cerchi e i triangoli indicano le osservazioni del dataset appartenenti a due classi, mentre le aree blu e rosse indicano gli esiti della classificazione}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Risultato di un regressore k-NN, per $k$ pari a 3. I cerchi indicano le osservazioni del dataset, le stelle verdi indicano il nuovo elemento da predire e le stelle blu gli esiti della predizione.}}{6}{figure.1.4}}
\newlabel{fig:knn_regressor}{{4}{6}{Risultato di un regressore k-NN, per $k$ pari a 3. I cerchi indicano le osservazioni del dataset, le stelle verdi indicano il nuovo elemento da predire e le stelle blu gli esiti della predizione}{figure.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.2}Modelli lineari}{6}{subsubsection.1.1.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Linear regression}}{7}{figure.1.5}}
\newlabel{fig:linear_regression}{{5}{7}{Linear regression}{figure.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.3}Support Vector Machine}{7}{subsubsection.1.1.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Possibili iperpiani che dividono lo spazio (a sinistra) e iperpiano ``migliore" (a destra)}}{8}{figure.1.6}}
\newlabel{fig:svc}{{6}{8}{Possibili iperpiani che dividono lo spazio (a sinistra) e iperpiano ``migliore" (a destra)}{figure.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Dati non divisibili linearmente}}{9}{figure.1.7}}
\newlabel{fig:svc_non_linear}{{7}{9}{Dati non divisibili linearmente}{figure.1.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.4}Alberi di decisione}{9}{subsubsection.1.1.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Dati visti sull'asse z}}{10}{figure.1.8}}
\newlabel{fig:svc_on_z_axis}{{8}{10}{Dati visti sull'asse z}{figure.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Overfitting dei dati con l'albero di decisione}}{11}{figure.1.9}}
\newlabel{fig:overfit_decision_trees1}{{9}{11}{Overfitting dei dati con l'albero di decisione}{figure.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Non supervisionato}{11}{subsection.1.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Grafico di overfitting con l'albero di decisione. La curva nera rappresenta l'errore di precisione nella predizione di dati di allenamento mentre la curva rossa rappresenta l'errore sui dati di test}}{12}{figure.1.10}}
\newlabel{fig:overfit_decision_trees2}{{10}{12}{Grafico di overfitting con l'albero di decisione. La curva nera rappresenta l'errore di precisione nella predizione di dati di allenamento mentre la curva rossa rappresenta l'errore sui dati di test}{figure.1.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Semi-supervisionato}{13}{subsection.1.1.3}}
\citation{ruder}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Apprendimento con rinforzo}{14}{subsection.1.1.4}}
\newlabel{fig:rl_scenario}{{1.1.4}{15}{Apprendimento con rinforzo}{subsection.1.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Scenario RL}}{15}{figure.1.11}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Ridimensionamento delle funzionalit\IeC {\`a}}{15}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}PCA (Principal Component Analysis)}{16}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}t-SNE (t-distributed stochastic neighbor embedding)}{17}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Induzione di insiemi fuzzy}{19}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:prova}{{2}{19}{Induzione di insiemi fuzzy}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}La logica fuzzy}{19}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Gli insiemi fuzzy}{19}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Possibilearn}{20}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Calcolo dell'insieme fuzzy}{21}{subsection.2.3.1}}
\newlabel{eq:1}{{1}{21}{Calcolo dell'insieme fuzzy}{equation.2.3.1}{}}
\newlabel{eq:2}{{2}{21}{Calcolo dell'insieme fuzzy}{equation.2.3.2}{}}
\newlabel{eq:3}{{3}{21}{Calcolo dell'insieme fuzzy}{equation.2.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Configurazione degli iperparametri}{23}{subsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Esempio di Linear fuzzifier ($\mathaccentV {hat}05E\mu _{\text  {lin}}$)}}{23}{figure.2.12}}
\newlabel{fig:fuzzifier}{{12}{23}{Esempio di Linear fuzzifier ($\hat \mu _{\text {lin}}$)}{figure.2.12}{}}
\bibcite{intro_machine_learning}{1}
\bibcite{towards_data_science}{2}
\bibcite{ruder}{3}
\bibcite{knuthwebsite}{4}
