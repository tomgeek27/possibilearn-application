Capitolo 1: L'apprendimento automatico

    Il Machine Learning insegna ai computer a compiere attività in modo naturale come gli esseri umani o gli animali: imparando dall’esperienza. In sostanza, gli algoritmi di Machine Learning usano metodi matematico-computazionali per apprendere informazioni direttamente dai dati, senza modelli matematici ed equazioni predeterminate. Gli algoritmi di Machine Learning migliorano le loro prestazioni in modo “adattivo” mano a mano che gli “esempi” da cui apprendere aumentano. Cerchiamo allora di capire cos’è il Machine Learning, come funziona e quali sono le sue applicazioni.

    Sezione 1.1: Approcci

    I tipi di algoritmi di machine learning differiscono nel loro approccio, nel tipo di dati che inseriscono e producono e nel tipo di attività o problema che devono risolvere. Il machine learning può generalmente essere suddiviso in due macro categorie: supervisionato, non supervisionato. A queste viene spesso aggiunta anche una terza che si chiama "apprendimento con rinforzo".

        Sezione 1.1.1: Supervisionato

        L'approccio supervisionato è una tecnica che prevede di lavorare su un insieme di dati etichetti dall'utente, da cui possa imparare a riconoscerne le varie differenze e in seguito a fare una predizione sulla possibile etichetta da attribuire. Questa tecnica può fornire due diversi tipi di risultati: discreti o continui. Per comprendere meglio questo concetto proviamo a fare un esempio.

        Parliamo di diagnosi mediche di una serie di pazienti. Quindi analizzando le diagnosi, un medico è in grado di definire se il paziente è in salute o meno. Da qui possiamo estrapolare quindi due insiemi/etichette differenti per il nostro caso: "in salute" e "malato". Fornendo come input ad un classificatore questo insieme di dati con le rispettive etichette appena definite, la macchina, tramite l'algoritmo, sarà in grado di fornire delle predizioni sulla possibile etichetta da attribuire ad ogni nuova diagnosi. Alle prime iterazioni vi è un'alta probabilità che l'errore nella predizione sia alta (a causa, ad esempio, di outlier, i quali rischiano di "confondere" l'algoritmo durante l'apprendimento). Proprio per questo è necessario fare diverse iterazioni, in modo che l'algoritmo capisca dove ha sbagliato (andando a confrontare la predizione con l'etichetta effettiva) e "aggiusta", di conseguenza, la predizione. Viene quindi naturale pensare che maggiore sia la mole di dati (le diagnosi mediche nel nostro caso), maggiore sarà il numero di casi in cui si può etichettare un elemento con maggiore precisione, proprio perchè impara da tutti i possibili casi presentatigli.
        In questo esempio abbiamo utilizzato solamente le classi "in salute" e "malato", ma nulla ci vieta di definirne una terza o una quarta, ad esempio possiamo etichettare un paziente come "in salute", "malato" e "malato terminale".

        Nel caso in cui, non vogliamo avere solo una classificazione della salute del paziente, ma preferiamo quantificare l'aspettativa di vita del paziente, non è più possibile ricorrere a dei classificatori. Da qui nasce la necessità di passare da un valore discreto ad un valore continuo: i regressori. 
        I regressori servono nel momento in cui si vuole quantificare un certo oggetto. Riprendendo l'esempio precedente, potremmo voler quantificare i tempi di guarigione di un malato, data una specifica diagnosi.
        
        Nei modelli di machine learning supervisionati vogliamo che questi si comportino bene con dati nuovi, mai visti prima e sui quali venga fatta una predizione il più precisa possibile. Per assicurarci di questo, dobbiamo assicurarci che il modello stiano lontano dall'overfitting (sovra-adattamento) e dell'underfitting (sotto-adattamento), vediamoli in dettaglio.
        L'overfitting consiste nell'adattare il modello in maniera eccessiva ai dati che gli sono stati forniti per allenarsi, il che non permetterebbe di generalizzare bene il modello per i nuovi dati mai visti prima perchè anche con un piccolo scostamento da quelli che sono i vincoli che determinano la predizione, comporterebbe una predizione sbagliata.
        L'underfitting invece, consiste in tutto il contrario dell'overfitting, ovvero nell'allenare il modello su delle regole troppo semplici e poco robuste, il che comporterebbe un modello che effettua delle predizioni su regole troppo vaghe.
        
/*        la funzione generalizzi l'insieme dei punti mantendo una soglia di errore non eccessiva. Questo perchè se la funzione approssima in maniera troppo precisa (overfitting) all'insieme dei punti, si rischia che i futuri elementi che verranno analizzati vengano predetti nel modo sbagliato, proprio perchè la funzione si è adattata in maniera eccessiva al campione inziale su cui l'alogirmto si è allenato. Se invece l'errore è eccessivo (underfitting) singificherebbe che la funzione non si avvicina nemmeno ai dati iniziali, quindi, anche qui, i futuri elementi da predire saranno errati.*/

        [TABELLA DI DATA CUSTOMER t1.0]

        Per un esempio più concreto su quali siano i problemi generabili dall'overfitting e l'underfitting vediamo la tabella [t1.0] dove si hanno dei dati in merito a delle persone. Supponiamo di voler predire se il cliente X vorrà acquistare una barca. Analizzando la tabella piuttosto attentamente si può notare che secondo la regola

        "Se un cliente ha meno di 45 anni, ha meno di 3 figli o non è divorziato, allora lui vorrà comprare una barca",

        secondo la quale, le predizioni (su questo dataset) saranno giuste al 100%! Ma questo singificherebbe anche che se in futuro un cliente C che volesse comprare la barca non rispettasse la regola (magari perchè ha semplicemente 46 anni o perchè ha 4 figli), la predizione del sistema sarebbe "C non vuole comprare una barca", il che sarebbe quindi errato. Questo è il caso dell'overfitting: stabilire le regole di predizione su troppi dati, in maniera troppo rigida.
        
        [IMMAGINE DI OVERFITTING] (1.1)

        Lo stesso si può fare al contrario, ossia quando le regole di predizione adottate dal sistema si basano su troppi pochi dati e sono troppo vaghe: facciamo un esempio.
        Supponiamo che il sistema identifichi un cliente come possibile acquirente di una barca se segue la seguente regola:

        "Se un cliente ha una casa allora vorrà comprare una barca"

        è naturale, leggendo la regola, pensare che questa sia sbagliata. Questo è il caso dell'underfitting, ossia il caso in cui si definiscono un modello che segue regola poco solide e troppo semplici.

        [IMMAGINE DI UNDERFITTING] (1.2)

        Quindi se il modello è sovra-adatttato al dataset di allenamento oppure sotto-adattato, si rischia di avere una algoritmo di scarso rendimento. Tuttavia esiste un hotspot che si trova a metà tra l'overfitting e l'underfitting, il quale permette di ottenere il miglior rendimento nella totale generalità dei casi. Questo è quello che si vuole ottenere da un algoritmo di apprendimento automatico.

        Il trade-off tra l'overfitting e l'underfitting è illustrato in figura 1.3

        [IMMAGINE DI TRADE-OFF 1.3]

            *** Sezione 1.1.1.1: k-Nearest Neighbor

            Vediamo nello specifico uno dei più semplici algoritmi di machine learning: k-Nearest Neighbor. Questo è un algoritmo utilizzato sia per la classificazione che per la regressione. In entrambi i casi l'algoritmo si basa sul un parametro fissato k il quale indica il numero di vicini da considerare.

            Supponiamo di avere due feature (per semplcità), 'a e 'b le quale descriveranno - insieme all'etichetta - ogni record del nostro dataset. 

            Nel caso di classificazione tramite il k-NN, un nuovo elemento non ancora etichettato, (il quale avrà le proprie coordinate ('a, 'b)), verrà classificato in base al tipo predominante dei suoi vicini. La scelta del k, è quindi l'unica, ma fondamentale scelta per determinare la precisione nella predizione dei futuri elementi da classificare.

            In figura xxx viene mostrato come influsice la scelta di differenti parametri k su uno stesso campione C.

            [IMMAGINE DI KNN SU CAMPIONE C CON K = 1]
            [IMMAGINE DI KNN SU CAMPIONE C CON K = 5]
            [IMMAGINE DI KNN SU CAMPIONE C CON K = 9]

            Questo algoritmo è utilizzato con un k dispari, il quale non permette di avere casi di indecisione e di poter sempre definire a quale classe appartiene.
            
            [IMMAGINE DI K-NN CLASSIFIER 1.4]

            Nel caso di regressione tramite il k-NN, il risultato sarà pari alla media del valore target che vogliamo predire di tutti i k vicini. Vediamo un esempio in dettaglio del funzionamento.

            Immaginiamo che nel nostro dataset abbiamo due caratteristiche 'a e 'b per ogni elemento, dove 'a è il valore su cui vogliamo basare il modello e 'b è il valore che siamo interessati a predire. Prendendo ad esempio, un k pari a 3 significherà (come vediamo in figura 1.5) che il nuovo elemento 'elem avrà come valore target la media dei target dei 3 elementi più vicini, sull'asse delle ascisse (ovvero l'asse su cui sono disposti i 'a)

            [IMMAGINE DI K-NN REGRESSOR 1.5]

            *** Sezione 1.1.1.2: Linear models

            I modelli lineari sono una classe di modelli che cercano di effettuare predizioni basandosi su una funzione lineare basata sull'insieme delle feature o caratteristiche appartenenti all'elemento da analizzare. 
            Nel caso della regressione, la funzione di cui parliamo è definibile come segue:

                y' = w[0] * x[0] + w[1] * x[1] + ... + w[n] * x[n] + b

            dove n rappresenta il numero di feature, x le feature, w i pesi da attribuire alle singole feature e la b rappresenta i parametri attribuiti al modello che si sta allenando.
            Prendendo una sola feature (quindi n pari a 1), y' risulterebbe:

                y' = w[0] * x[0] + b
            
            la quale è esattamente una funzione di una linea retta, dove w è l'angolo e b è lo scostamento dall'origine degli assi.

            [IMMAGINE DI UNA REGRESSIONE LINEARE OER REGRESSIONE] (1.0)

            Per riprendere l'esempio precedente, supponiamo che si voglia quantificare il numero di giorni necessari per guarire un paziente malato.
            Supponiamo, per semplcità, di avere una sola caratterisitica definita all'interno della diagnosi che rappresenta l'età del paziente (sull'asse delle ascisse) e per ogni punto il relativo tempo di guarigione - in giorni - (sull'asse delle ordinate). 
            Avendo quindi una serie di punti, è possibile tracciare una retta che approssima a tutti i punti definiti nel campione di allenamento (anche chiamato training set).
            Nell'immagino 1.0 è illustrata la retta appena citata.

            Tutto questo, per quanto riguarda l'utilizzo di modelli lineari nella regressione, nella classificazione invece si mantiene la stessa formula con la piccola differenza che si introducono degli intervalli per definire a quale classe appartiene il singolo caso. Nella classificazione binaria, ad esempio la formula risulterebbe come segue:

                y' = w[0] * x[0] + w[1] * x[1] + ... + w[n] * x[n] + b > 0
            
            dove, supponendo di avere le classi "1" e "0", se la y' fosse maggiore di 0 appartiene alla classe "1", alla classe "0" altrimenti.

            TODO: aggiungere la suddivisione in multiclassi
            
            [IMMAGINE DI REGRESSIONE LINEARE PER CLASSIFICAZIONE]

            Nella figura xxx è mostrato l'esempio appena citato, dove i cerchi sono la classe 0 e i triangoli la classe 1.

            *** Sezione 1.1.1.3: Support vector Classifier

            I SVC (Support Vector Classifier) sono una classe di modelli che si preoccupa di individuare un iperpiano utile a separare (e quindi classificare) i punti nel piano in diversi gruppi, i quali vanno a definire le classi che ci permettono, una volta inseriti i nuovi dati, di attribuire a questi, la rispettiva classe.

            Il primo problema che i SVC devono risolvere, è capire quale sia l'iperpiano che suddivide nel modo migliore i dati, questo perchè dati, ad esempio, due gruppi di punti è possibile dividerlo in infiniti modi. 
            Una volta presi gli iperpiani candidati, per decidere qual'è il migliore, si prendono i punti P più vicini a questo iperpiano I, i quali vengono definiti vettori di supporto. Per ogni retta R e i relativi vettori di supporto V, viene calcolata la distanza tra R e V la quale indicherà il "margine". Si definisce retta migliore, la retta che riesce a massimizzare il proprio margine dai vettori di supporto. In figura xxx è possibile vedere tutti i componenti dell'SVC in gioco.

            Tutto questo però accade quando abbiamo solamente due dimensioni. Quando ci confrontiamo con dei problemi reali, le feature in gioco che definiscono i punti sono molte, molte di più. Per ovviare a questo problema si ricorre alle funzioni kernel, le quali sono delle funzioni in grado di mappare dei vettori definito in un spazio a n dimensioni, in uno spazio a m dimensioni.
            Questo 'trick' del kernel consente di riutilizzare i SVC anche laddove non è possibile suddividere i punti tramite una semplice retta.

            Supponiamo di avere un caso mostrato in figura xxx, dove si hanno due sole dimensioni, ma dove non è possibile suddividere i punti con una semplice linea.


            Tramite una funzione kernel trasformiamo questi due punti definiti dalle coordinate x e y, in dei punti definiti dalle coordinate x, y e z, in questo modo possiamo rivedere il tutto in 3 dimensioni e tracciare un iperpiano che suddivide correttamente i punti nello spazio a 3 dimensioni, per poi successivamente ridefinirlo secondo le due dimensioni iniziali di partenza.

            Come esplicato in un articolo di towards data science (https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989), riporto l'esempio esplicativo:
            
            Presi dei punti in uno spazio bidimensionale, ci capita la situazione in cui questi non sono suddivisibili tramite una retta.

            [IMMAGINE DEI PUNTI DISPOSTI IN MANIERA NON DIVISIBILE IN UN PIANO A 2 DIMENSIONI]

            Aggiungiamo una terza dimensione z e definiamola come segue:

                z = x**2 + y**2

            Il risultato ottenuto è dato dalla seguente immagine:

                [IMMAGINE DEI DATI PLOTTATI SULL'ASSE Z]
            
            Notiamo subito che ora è facile suddividere i punti in due distinti gruppi tramite una retta k (che rispetta il principio delle SVM):

                k = z
            
            dato che z è stato definito come:

                z = x**2 + y**2
            
            poniamo la retta k come segue:

                k = x**2 + y**2
            
            il che ci consente di ottenere una linea nello spazio a due dimensioni, che sarà esattamente la nostra divisione tra i gruppi.

            Quindi quando ci si trova davanti a problemi n-dimensionali bisogna sempre ricorrere al trucco del kernel. Questo trucco prevede anche dei parametri C e Gamma, detti parametri di tuning.
            Essi influiscono sulla selezione dell'iperpiano che si va ad ottenere nello spazio inziale:
            *C: è il parametro che consente di definire un costo nell'errore della suddivisione dei punti, vale a dire che nel caso in cui si scelga una C grande significa che ogni singolo errore avrà un costo elevato, andando cosi ad adattare il modello SVM il più preciso possibile al training set. Adattando il modello in maniera eccessiva al training set, però, si rischierebbe di andare in overfitting e quindi comporterebbe ad una classificazione errata nel caso di esempi estranei all'insieme dei dati di allenamento. Avendo invece, una C piccola, significa che il costo di errore sarà basso, il quale comporterà che durante l'allenamento del modello, saranno presenti diversi errori di classificazione, però, così facendo si sta generalizzando maggiormente il modello a nuovi casi da etichettare, il che può essere positivo. Come citato sopra, vi rimando al grafico che denota che lo sweetspot, ossia il compromesso per ottenere un buon risultato di predizione, è a metà tra l'underiftting e l'overfitting.

            *Gamma: TODO

            *** Sezione 1.1.1.5: Alberi di decisione

            Gli alberi di decisione o decision trees, è un algoritmo di classificazione e regressione, che, fondamentalmente, basa la sua logica sull'apprendimento di una struttura, sulla domanda 'se questo allora...'/'altrimenti questo...' fino ad arrivare ad una decisione finale.

            Prendo spunto da un esempio fatto nel libro 'Introduction to machine learning' in cui si vuole distinguere un animale tra: falco, pinguino, delfino, orso.
            L'algoritmo lavora andando a fare determinate domande a cui puoi rispondere vero o falso, ad esempio, si parte da una domanda che può semplicemente essere: "Ha le piume?". In questo modo si aprono due strade, "ha le piume" e "non ha le piume", suddividendo in due gruppi distinti gli animali. Prendendo gli animali che non hanno le piume (orso e delfino), e facendo un'ulteriore domanda, che differenzia i due animali, si può arrivare a capire di quale animale si sta parlando tramite questa serie di risposte. Seguendo la domanda, "Ha le pinne?", possiamo capire che se la risposta è si, è il delfino, ovvero l'unico animale tra i 4 sopra scelti che non ha le piume e ha le pinne.

            Seguendo questa logica è possibile arrivare (con le giuste domande) a dire di quale tipo di animale si sta parlando.

            Questo era un semplice esempio che non rispecchia la realtà, e i campi di utilizzo di questo algoritmo. Spesso i dati che vengono analizzati hanno dei valori di tipo continuo, quindi la domanda a cui si risponde, tendenzialmente, è del tipo: "x è maggiore di y?". Questo è il modo in cui operano questi algoritmi in caso di valori di tipo continuo e così facendo si va a costruire un completo albero di decisione.

            Come abbiamo già visto anche negli altri algoritmi, il problema dell'overfitting e underfitting è un problema ricorrente e nel caso dell'albero di decisione non è da meno.
            Infatti se viene costruito un albero troppo dettagliato e quindi con un livello di profondità eccessivo si va ad adattare in maniera eccessiva (quindi in overfitting) al training set, andando a definire zone anche molto piccole, come viene mostrato in figura xxx.

            [IMMAGINE DECISIONN TREE OVERFITTING]

            Analizzando un caso simile è possibile vedere in figura xxx come l'errore su un test set (quindi un insieme di dati utile a testare il modello allenato), cresca rispetto al train, questo succede proprio perchè il modello non è generalizzato, ma si è allenato specificandosi in maniera eccessiva al training set.

            [IMMAGINE GRAFICO ERRORE SU TEST SET CON OVERFITTING]

            Per risolvere questo problema, esistono due strategie:
            * far terminare lo sviluppo dell'albero dopo pochi passi (pre-potatura): limitare la profondità dell'albero definendo una variabile che tenga conto della prodfondità dell'albero, la quale quando supera una soglia prefissata, interrompe lo sviluppo di questo
            * rimuovendo i nodi che hanno contengono informazioni ridotte (post-potatura)

        Sezione 1.1.2: Unsupervised

        Il secondo importante approccio all'applicazione del machine learning, consiste nella tecnica non supervisionata. Cosa si intende dire con non supervisionata? Come può una macchina imparare se nessuno la guida nella scelta di decisioni?
        Questa è proprio la sfida che si vuole superare con questa tecnica, ovvero far estrapolare alla macchina, delle informazioni "nascoste" all'interno dei dati che gli vengono forniti. Cercando dei legami o regole che i dati tendono a seguire, all'interno del set datogli in input senza fornire nessun tipo di ifnormazione ulteriore che possa guidare il modello di apprendimento in qualche modo (a differenza, appunto, del modello supervisionato).

        Ci sono diversi tipi di machine learning non supervisionato, in questo capitolo ci limiteremo a elencarne la logica che seguono e quale possibile campo di utilizzo, senza entrare troppo nello specifico.

        Un primo tipo di Unsupervised learning è il clustering. Il clustering è una tecnica che serve per suddividere in gruppi distinti degli elementi che hanno dati e caratteristiche in comune. Per essere più chiari, vediamo subito un possibile esempio.
        Supponiamo di aver scattato una serie di foto i cui soggetti sono delle persone, magari nostri amici o parenti e decidiamo di caricarle su un social network. Durante il caricamento, il social su cui le stiamo caricando, si permette di visionare le foto e applicare proprio un algoritmo di clustering. In che modo? L'algoritmo non sa nè chi siano le persone raffigurate nè quante esse siano. L'algoritmo andrà a cercare tutti i volti nelle foto che abbiamo caricato e successivamente, dopo aver ottenuto una lista di tutti i volti estrapolati da ogni foto, tramite un algoritmo di clustering va a cercare somiglianze in questi volti, andando così a raggruppare le foto dove è presente lo stesso soggetto.

        ***TODO***: ulteriori esempi

       ***  Sezione 1.1.3: Semi-supervised

        L'approccio semi-supervised, non è un vero e proprio approccio, bensì una tecnica che sta a metà tra le due appena viste (supervisionato e non supervisionato). Questa tecnica consiste nel combinare le due tecniche e quindi fornire un risultato basandosi su un input eterogeneo: etichettato e non.

        Questo approccio risulta utile quando si ha una grande mole di dati, e gli utenti che sono in grado di etichettare i dati sono utenti specializzati, quindi, nella situazione reale, non sempre questo è possibile, proprio perchè possono mancare risorse umane competenti o tempistiche adeguate.

        Esistono differenti algoritmi per l'apprendimento automatico mediante un sistema semi-supervisionato:
        * Self training
        * Multi-view training
        * Self-ensembling 

        Per quanto riguarda il multi-view training, esso mira a formare diversi modelli con diverse visualizzazioni dei dati. Idealmente, queste viste sono complementari e i modelli possono collaborare per migliorare il risultato finale. Queste viste possono differire in diversi modi, ad esempio nelle funzionalità che utilizzano, nelle architetture dei modelli o nei dati su cui i modelli vengono formati.

        Il self-ensembling, come il multi-view training, punta a combinare diverse varianti dei modelli. A differenza però di quest'ultimo, la diversità nei modelli non è un punto chiave perchè il self-ensembling utilizza principalmente un singolo modello in diverse configurazioni al fine di rendere più affidabili le previsioni del modello. 

        A titolo di esempio vedremo più in dettaglio l'algoritmo di "Auto-allenamento" che è stato uno dei primi ad essere sviluppato ed è l'esempio più diretto di come le previsioni di un modello possono essere incorporate nel training del modello.

        L'algoritmo di auto-allenamento prevede quindi di basarsi per quanto può su dei dati che sono stati preventivamente definiti secondo un particolare modo e altri che sono solamente dei dati da studiare e analizzare. Questi ultimi vengono comuqnue utilizzati, ma in maniera più cauta, ovvero, prima di allenare il modello per imparare a fare una predizione sui futuri elementi dati in input, il modello si concentrerà ad etichettare gli input che ancora sono senza alcuna etichetta.

        Come viene spiegato in un articolo su ruder.io (https://ruder.io/semi-supervised/index.html#relatedmethodsandareas), la logica di classificazione dei dati non ancora classificati segue quanto scritto:
        "Formalmente, l'auto etichettamento avviene su un modello M avente un training set etichetto L con delle etichette contenute in C e un set non etichettato U. Ad ogni iterazione, per ogni x in U, il modello fornisce delle predizioni su m(x) sottoforma di probabilità p(x, c) ovvero probabilità che x appartenga alla classe c per ogni c in C. Tra le probabilità appena calcolate, definiamo P(x, c) come la probabilità avente il valore maggiore, allora se P è maggiore di una soglia T, x verrà aggiunto a L con l'etichetta c. Questo processo viene ripetuto per un numero fisso di iterazioni o fino a quando non ci sono più dati da etichettare.".[nota: traduzione libera del testo: nell'originale è presente una formula ceh io rendo a parole]

        Di seguito vediamo uno pseduo-codice che segue quanto detto sopra:

        \usepackage{algorithm}
        \usepackage[noend]{algpseudocode}
        \usepackage{amsmath}
        \usepackage{amsfonts}
        \begin{algorithmic}[1]
            \Repeat
                \State $m \gets train\_model(L)$
                \For {$x \in U$}
                    \If {$\max m(x) > \tau$}
                        \State $L \gets L \cup \{(x, p(x))\}$
                    \EndIf
                \EndFor
            \Until {no more predictions are confident}
        \end{algorithmic}
        
        Sezione 1.1.4: Reinforcement learning

        Il quarto ed ultimo approccio chiamato Reinforcement learning, è un approccio che si differenzia da quelli visti fino ad ora. Questo paradigma si occupa di problemi di decisioni sequenziali, in cui l'azione da compiere dipende dallo stato attuale del sistema e ne determina quello futuro. In altre parole, questo è un sistema dinamico che può apprendere in seguito ad ogni decisione presa, a prescindere che questa sia giusta o sbagliata.

        [IMMAGINE DELLO SCENARIO DI RL] (https://www.aitrends.com/education/udacitys-school-of-ai-opens-the-new-deep-reinforcement-learning-nanodegree-program-for-enrollment/) (https://www.guru99.com/reinforcement-learning-tutorial.html)

        Quando il sistema prende una decisione, esso otterrà una "ricompensa" che può essere un punteggio, che sarà alto o basso a seconda se la decisione presa è giusta o sbagliata. Con questa logica, la macchina cercherà di fare sempre meglio per arrivare a ottenere il punteggio più alto possibile, prendendo così solo le decisioni corrette. 

        Di seguito, sono citati dei casi di utilizzo di questo approccio:

        * Imparare a giocare a scacchi
        * Imparare a guidare un veicolo

    Sezione 1.2: Ridimensionamento delle feature

    Nel machine learning, come abbiamo visto, l'input, gioca un ruolo fondamentale nello sviluppo di un modello, che riesca a predire nel modo corretto i nuovi dati che verrano esaminati da quest'ultimo.
    In questo testo ci concentreremo proprio sul riconoscimento dei volti all'interno delle immagini, quindi per per parlare dell'importanza del ridimensionamento delle feature, faremo riferimento proprio all'analisi di immagini che raffigurano persone, animali o oggetti.

    Spesso, ci troviamo davanti a dei dati di dimensioni eccessive, i quali comportano in primis dei problemi a livello tempistico e anche a problemi a livello computativo.
    
    Ci basti pensare che quando si cerca di analizzare un'immagine per estrapolarne delle informazioni (riconoscimento di oggetti, persone, animali) dobbiamo prendere passare in rassegna tutti i pixel! Supponiamo di prendere anche un'immagine, a bassa risoluzione, ad esempio 500x500, significherebbe trovarsi davanti a 500**2 pixel, ovvero 250.000 elementi per una singola immagine! Questo comporterebbe quindi, di analizzare uno spazio sovra dimensionato, con, appunto 250.000 dimensioni.

    Pensare ad uno spazio di quelle dimensioni è impensabile, proprio per questo ci vengono in soccorso delle tecniche che si occupano di ridurre il numero di elementi che definsicono l'oggetto, senza perdere, o meglio, estrapolando, solamente le informazioni più utili che permettono di differenziare un oggetto da un'altro.

    Pensiamo ad esempio a un'immagine in cui è raffigurato il volto di una persona. E' normale pensare che non tutti i pixel siano di fondamentale importanza per riconoscere il soggetto raffigurato. Ad esempio, tutti i pixel presenti nei bordi dell'immagine saranno sicuramente da scartare in quanto non ci diranno niente sulla persona raffigurata, così come molti altri pixel che raffigurano parti poco interessanti, ad esempio lo sfondo dell'immagine. Vediamo nel dettaglio quali sono gli strumenti più utilizzati per risolvere questo tipo di problemi.

    *** Sezione 1.2.1: PCA (Principal Component Analysis)

    PCA (Principale Component Analysis) è un metodo di riduzione delle dimensionalità. Lo scopo di PCA è quindi quello di diminuire il numero di variabili, limitando il più possibile la perdita di informazioni. 
    In primo luogo, viene calcolata la media per ogni feature. Una volta calcolata, il vettore risultante lo faremo coincidere con l'origine degli assi, in questo modo ogni punto verrà traslato di conseguenza. A questo punto viene calcolata la retta che meglio si adatta a tutti i punti, ovvero la retta R che minimizza la somma delle distanze dei punti da R. Questa viene chiamata principal component 1. Si ripete questo passaggio per ogni dimensione, mantendo la perpendicolarità della nuova retta (o principal component_{n}) rispetto all'ultima retta calcolata (o principal component_{n-1}). 
    Una volta calcolate tutte le principal component dello spazio PCA si va a calcolare, per ognuna di esse, l'autovettore, il quale sarò utilizzato per determinare la nuova posizione di ogni punto sul rispettivo asse. Una volta scalati tutti i punti si può calcolare la varianza per ogni asse. Il valore della varianza x, calcolata in percentuale, ci dice quanto pesa l'inforamzione contenuta sull'asse x, questo ci permetterà quindi, di eliminare gli assi meno interessanti, ovvero gli assi con la varianza più bassa. 

    Questa tecnica, oltre a semplificare il lavoro di manipolazione delle caratteristiche, aiuta a migliorare i risultati dell'algoritmo di machine learning. PCA aiuta gli algoritmi di machine learning perchè estrpola le informazioni realmente utili per predire la classe o il valore da attrivuire ad un oggetto. Tutte le informazioni di contorno, come ad esempio, i pixel situati sul bordo di un'immagine posso essere fuorvianti per l'algoritmo di apprendimento. Questo è il motivo per il quale esso semplifica e ottimizza i valori risultanti.

    PCA vede un vasto utilizzo nell'ambito di:
    \begin{itemize}
        \item riconoscimento facciale
        \item image compression
        \item rilevamento di pattern in campi ad alta dimensionalità
        \item data mining
    \end{itemize}

    *** Sezione 1.2.2: t-SNE (t-distributed stochastic neighbor embedding)
    %Documenta con delle immagini prendedole dal video https://www.youtube.com/watch?v=NEaUSP4YerM
    È una tecnica di riduzione della dimensionalità non lineare che si presta particolarmente alla mappatura di spazi ad alta dimensionalità in uno spazio a due o tre dimensioni, nel quale possono essere visualizzati tramite un grafico di dispersione. L'algoritmo modella i punti in modo che oggetti vicini nello spazio originale risultino vicini nello spazio a dimensionalità ridotta, e oggetti lontani risultino lontani.

    Per spiegare il funzionamento di questo algoritmo, basiamoci su un caso semplice: un grafico 2D. Con questo esempio spiegheremo quindi il funzionamento di t-SNE e come è possibile passare da due dimensioni, ad una sola, mantenendo le corrette distanze. Per farlo ci baseremo sulle probabilità che un elemento sia vicino ad un'altro. Quindi per ogni punto x, andiamo a centrare su di esso una curva gaussiana. Per ogni altro punto y app INSIEME / x, andiamo a inserirlo sotto la distribuzione gaussiana, per poi calcolarne la probabilità di densità.
    //la quale verrà utilizzata per calcolare la probabilità che ogni altro elemento, sia vicino a x. 
    Si usa la curva gaussiana perchè lavora bene su casi come questo: restituisce un'alta probabilità se un elemento è molto vicino e una molto bassa probabilità se questo è lontano. A questo punto normalizziamo la curva per tutti i punti in modo che essi abbiano una misura proporzionata e non indipendente. La distribuzione può in realtà essere manipolata tramite una variabile chiamata perplessità, la quale va a modificare la varianza e quindi l'ampiezza della curva.
    A questo punto avremo ottenuto una matrice M1 quadrata con tutti gli score per ogni coppia. Ora andiamo a inserire tutti i punti, in maniera randomica su un'unico asse. Analogamente a quanto appena fatto, calcoliamo la probabilità di vicinanza tra i punti, con la differenza che questa volta useremo la distribuzione di Student (in inglese t-distribution, da cui deriva la t di t-SNE). Analogamente, otteremo una seconda matrice M2 quadrata, la quale sarà, probabilmente, molto diversa da M1. L'obiettivo adesso, sarà quello di adattare la matrice M2 a M1. Così facendo riusciremo a mantenere i cluster visualizzabili nel grafico bidimensionale.

    Nella seconda parte dell'algoritmo, viene usata una t-distribution perchè distacca meglio i cluster nel piano generato. Se avessimo usato una distribuzione gaussiana, come nella prima parte, il risultato ottenuto sarebbe stato meno visibile, in quanto tutti i cluster si sarebbero ammassati al centro.
    
    L'ampiezza della curva gaussiana per un elemento x si basa sulla densità degli elementi vicini, questo implica che lo score che A sia vicino a B, può essere diversa dallo score che B sia vicino ad A. Per gestire questo problema viene calcolata la media tra i due. 
    

    Sezione 1.2: Utilizzi

    Machine Learning: le applicazioni
    Le applicazioni di Machine Learning sono già oggi molto numerose, alcune delle quali entrate comunemente nella nostra vita quotidiana senza che in realtà ce ne rendessimo conto.

    Pensiamo per esempio all’utilizzo dei motori di ricerca: attraverso una o più parole chiave, questi motori restituiscono liste di risultati (le cosiddette SERP – Search Engine Results Page) che sono l’effetto di algoritmi di Machine Learning con apprendimento non supervisionato (forniscono come output informazioni ritenute attinenti alla ricerca effettuata in base all’analisi di schemi, modelli, strutture nei dati).

    Altro esempio comune è legato ai filtri anti-spam delle e-mail basati su sistemi di Machine Learning che imparano continuamente sia ad intercettare messaggi di posta elettronica sospetti o fraudolenti sia ad agire di conseguenza (per esempio eliminandoli prima che vengano distribuiti sulle caselle personali degli utenti). Sistemi di questo tipo, anche con sofisticazioni maggiori, vengono per esempio impiegati anche nel settore Finance per la prevenzione delle frodi (come la clonazione della carta di credito), dei furti di dati e identità; gli algoritmi imparano ad agire mettendo in correlazione eventi, abitudini degli utenti, preferenze di spesa, ecc.; informazioni attraverso le quali riescono poi a identificare in real-time eventuali comportamenti anomali che potrebbero appunto identificare un furto od una frode.

    Interessanti esempi di Machine Learning con apprendimento supervisionato arrivano dal settore della ricerca scientifica in campo medico dove gli algoritmi imparano a fare previsioni sempre più accurate per prevenire lo scatenarsi di epidemie oppure per effettuare diagnosi di tumori o malattie rare in modo accurato e tempestivo.

    E ancora, sempre nell’ambito dell’apprendimento supervisionato, ci sono interessanti applicazioni di Machine Learning a livello di riconoscimento vocale o identificazione della scrittura manuale.

    Come accennato, i sistemi che si basano sull’apprendimento con rinforzo stanno alla base dello sviluppo delle auto a guida autonoma che, proprio attraverso il Machine Learning, imparano a riconoscere l’ambiente circostante (con i dati raccolti da sensori, GPS, ecc.) e ad adattare il loro “comportamento” in base alle specifiche situazioni che devono affrontare/superare.

    Anche i cosiddetti sistemi di raccomandazione sfruttano il Machine Learning imparando dal comportamento e dalle preferenze degli utenti che navigano su siti web, piattaforme o applicazioni mobile; ne sono un esempio quelli che comunemente ci siamo abituati a vedere ed utilizzare sulle piattaforme di eCommerce come Amazon o di intrattenimento e accesso a contenuti come Netflix o Spotify.

/*******************************************************************************************************************************************************************************************************/
Capitolo 2: Induzione di insiemi fuzzy

    Sezione 2.1: La logica fuzzy

        La logica fuzzy (in italiano, logica sfocata), è un'estensione della logica booleana. Nella matematica booleana sono presenti solo due valori attribuili alle variabili: vero e falso. 
        La logica fuzzy, si definisce "estensione" della logica di Boole in quanto, al posto di prevedere solamente due possibili valori, viene previsto un insieme di valori continui compresi nell'intervallo [0, 1]. In questo intervallo, lo 0 corrisponde di fatto, al valore "falso" e 1 a "vero" della logica booleana. Con questa estensione oltre a poter dire "vero" o "falso", è possibile dire, tramite il valore di appartenenza (o grado di verità), quanto è vera una proprietà.

        Quindi, data una proprietà P, e un elemento x, si può dire:
            x rispetta P con valore y

        dove y è compreso nell'intervallo [0,1].

        Per fare un esempio più concreto, si può pensare a tutte quelle cose che sono relativamente soggettive, in cui non esiste solo bianco o nero, bensì ci sono delle vie di mezzo, più o meno vere.
        Supponiamo di prendere un oggetto di cui predichiamo "essere freddo", si può dire che:
        * un gelato "è freddo" con valore (o grado di verità) uguale a 0.9
        * un bicchiere d'acqua a temperatura ambiente "è freddo" con valore uguale a 0.4
        * la resistenza di una lampadina accesa "è fredda" con valore uguale a 0.1

        La logica fuzzy è strettamente legata anche alla matematica degli insiemi. Approfondiamo questo argomento nella prossima sezione.

    Sezione 2.2: Gli insiemi fuzzy

        Gli insiemi fuzzy sono un'estensione della classica teoria degli insiemi, dove si ha che un elemento può appartenere o meno ad un insieme. Con questa estensione, analogamente al grado di verità per la logica booleana, si definisce un valore x di appartenenza ad un insieme, compreso nell'intervallo [0, 1]. Per x pari a 1 si ha che l'elemento è certamente incluso nell'insieme, per x pari a 0 si ha che l'elemento è sicuramente escluso dell'insieme, per tutti i valori compresi si ha una appartenenza che può essere più o meno forte.

        Per fare un esempio, definiamo lo spazio U come uno spazio dove gli elementi che ne fanno parte sono delle persone ed un'insieme A che racchiude tutti le persone giovani.
        Prendendo le seguenti persone di U:
        * neonato
        * ventenne
        * ottantenne
        si può definire per ognuna di essere un grado di appartenenza all'insieme A. Ad esempio:
        * neonato appartiene ad A con un valore pari a 1
        * ventenne appartiene ad A con un valore pari a 0.8
        * ottantenne apparteiene ad A con un valore pari a 0.1

        Formalizzando quanto appena detto, definiamo:
        % si può definire una funzione del grado di appartenenza nu basata su un predicato p in uno spazio U tale che:

            nu_A: U -> [0,1]


        Dove nu_A rappresenta la funzione di appartenenza ad A ed U rappresenta lo spazio considerato. Un insieme fuzzy è definito dalle coppie (x, nu_A(x)), quindi dall'elemento x e il relativo grado di appartenenza ad A. Formalmente:
            A = { (x, nu_p(x)) | x appartiene ad U}

    Sezione 2.3: Possibilearn

        Nel capitolo precedente abbiamo visto le diverse tecniche utilizzate nel machine learning. L'algoritmo che andremo a descrivere ricade nell'approccio supervisionato, ovvero la tecnica che necessita dati preventivamente classificati per effettuare predizioni. 

        Questo algoritmo si basa sulla logica di induzione di insiemi fuzzy. In questo caso per classificazioni si intendono i gradi di appartenenza ad un certo insieme (fuzzy). 
        %TODO fai un esempio
        %OTOD definisci prima possibilearn
        %TODO citazione del testo del professore
        Dato un insieme X = {} e il relativo insieme di gradi di appartenenza NU = {}, l'obiettivo di possibilearn, consiste nel determinare la forma dell'insieme fuzzy tramite l'utilizzo di X e NU. Questo richiede di risolvere due problemi:
        * calcolare l'insieme fuzzy
        * definire i parametri per la funzione di appartenenza all'insieme fuzzy

        Sezione 2.3.1: Calcolare l'insieme fuzzy
            Partendo dalle ipotesi che:
            * A è l'insieme fuzzy che contiene tutti gli elementi con grado di appartenenza pari a 1. 
            Formalmente A viene definito come segue:
                A = {x appartente X | nu_A(x) = 1}
            dove X è l'insieme di tutte le immagini dei punti dello spazio originale che sono mappate tramite una funzione phi, i quali appartengono ad una ipersfera di centra alfa e raggio R.

            * Il grado di appartenenza nu_A(x) dipenderà solo dalla distanza di phi(x) da alfa.

            Prese queste due ipotesi è possibile definire il problema come:

            "Trovare la più piccola ipersfera avente centro alfa e raggio R, la quale racchiude tutte le x in X per cui vale che nu_A(x) = 1"

            Tradotto in un modello matematico, questo diventa un problema di ottimizzazione, la cui funzione obiettivo è:

            [FORMULA DI MINIMIZZAZIONE]

            dove C è una costante di cui parleremo successivamente, mentre epsilon e tau sono le variabili di scarto utilizzate nel problema di ottimizzazione.

            Epsilon è la variabile legata al posizionamento dei punti, all'interno dell'ipersfera.

            Tau, è come epsilon una variabile di scarto ma con la differenza che essa riferisce al posizionamento dei punti all'esterno della sfera.

            Alla funzione obiettivo descritta sopra, aggiungiamo i vincoli:

            [vincolo 1]
            [vincolo 2]
            [vincolo 3]

            dove possiamo notare che se il grado di appartenenza, vale a dire nu_{i} è pari a 1, otteniamo che nel vincolo 1 si ha:

            [vincolo 1 avente nu_{i} pari a 1]

            ossia che la distanza da alfa per x_{i} ([formual che descrive la distanza dal centro]) è minore o uguale al raggio della sfera ([formula del raggio]).

            Al contrario, se nu_{i} è pari a 0, otteniamo che nel vincolo 2 si ha:

            [vincolo 2 avente nu_{i} pari a 0]

            ovvero che la distanza dal centro della sfera(alfa) per x_{i} è maggiore al raggio dell'ipersfera ([formula del raggio])

            Passando per il duale e risolvendo i calcoli matematici, si può dimostrare che, preso qualunque punto x appartenente a X, una funzione kernel k, vale:
                [RISCRITTURA DI R^2 IN BASE A X]

            il che ci permette di calcolare la distanza tra il centro della sfera - nella soluzione ottimale - e phi(x).
            Analogamente al Support Vector Clustering, in possibilearn si individuano i support vector, ossia i punti che delimitano la zona del cluster. In questo caso i phi(x) che giaciono sulla superificie dell'ipersfera. Inoltre per tutti i punti x aventi nu_{A}(x) pari a 1 vale che la distanza di x da alfa (nella soluzione ottimale) è minore o uguale alla distanza di xi da alfa per tutti i support vector xi.
            Formalmente, definendo R*^2(x) la distanza di x dal centro della sfera (nella soluzine ottimale), possiamo riscrivere quanto appena detto come:
                
                R*^2(x) <= R*^2(xi) per ogni x | nu_{A}(x) = 1 e per ogni support vector xi .
            
            A questo punto bisogna definire come si comporta l'insieme fuzzy per tutti i punti, la cui distanza è maggiore del raggio dell'ipersfera. Per farlo dobbiamo prima definire dove questa distanza diventa sufficiente a dire che x è certamente escluso dall'insieme (fuzzy). In altre parole, bisogna trovare il confine oltre cui il grado di appartenenza è pari a 0. Sapendo che R*^2 varia tra un minimo e un massimo, prendiamo il massimo, ossia: max_{x} R*^2(x). Esso verrà utilizzato per definire la distanza, oltre il quale l'elemento sarà certamente escluso.

            Tutto quello che succede nel mezzo tra questi due confini dipende da una funzione chiamata fuzzifier. Le fuzzifier sono funzioni che possono assumere qualunque forma, le quali descrivono come si comporta il nu_{A}(x) al crescere della distanza di x dalla sfera. Questa, come la C definita precedentemente viene definita iperparametro. 
            
            Nella prossima sezione parleremo dell'hyper-parameter tuning (in italiano, ottimizzazione dei parametri), quella parte del machine learning che consiste nell'ottimizzazione degli iperparametri che influiscono sul modello di apprendimento automatico e quindi sulla relativa predizione.

            Configurazione degli iperparametri

            Iperparametro è una termine ricorrente quando si parla di machine learning. Questo si definisce iperparametro e non semplicemente parametro per differenziare i dati che sono autmoaticamente generati all'itnerno dell'algoritmo di ML rispetto ai dati che gli vengono forniti dall'esterno (quindi dall'utente). Questi iperparametri costituiscono i dettagli del modello, i quali influiscono sulle performance di questo.
            Abbiamo concluso il paragrafo precedente parlando di fuzzifier. Il fuzzifier è proprio un iperparametro. Fissato un insieme fuzzy A, 
            esso definisce come si comporta la funzione di appartenenza per tutte le x che hanno nu_{A}(x) < 1. Tramite questa funzione, quindi, siamo in grado di dire come descresce il grado di appartenenza ad A. Negli esperimenti fatti all'interno della tesi abbiamo usato delle classiche funzioni, come:

            * Linear
            * Quantile Linear Piecewise Fuzzifier

            riportate in figura.
            
            Un secondo iperparametro che vediamo in possibilearn è la costante C che vediamo nella funzione da ottimizzare:
            
            [FUNZIONE DA OTTIMIZZARE].

            Questa costante rappresenta un iperparametro il quale definisce il costo dell'errore nella classificazione degli elementi durante la costruzione dell'ipersfera. Questa C funziona in maniera analoga alla C nelle SVM (Support Vector Machine). Tanto più è grande la C, tanto più sarà grande il costo dell'errore nella classificazione della variabile i-esima. Al contrario, tanto più è piccola la C, tanto più piccola sarà il costo dell'errore. Come mostrato in figura, vediamo meglio il ruolo di questo iperparametro. Nella figura prendiamo una funzione di appartenenza f (o fuzzifier) fissata, e vediamo come al cambiare di C, la f si modifica. 
            Facendo crescere la C si aumenta il costo dell'errore, sapendo che la FO che vogliamo risolvere è una FO di minimo, bisogna mantenere il costo più basso possibile. Questo si traduce visivamente in una funzione "rigida" avente un cambiamento molto brusco. È facile notare che maggiore è la C e maggiore sarà la somiglianza di nu ad una funzione di appartenenza in un classico insieme.

            [FIGURA DI FUZZIFIER DI MALCHIODI]
            [FIGURA DI FUNZIONE DI APPARTENENZA DI UN INSIEME]

            Esiste un ulteriore parametro che modifica la forma del fuzzifier, questo è il kernel. Il kernel, come abbiamo visto nelle SVM è una funzione che viene utilizzata per mappare uno spazio N-dimensionale, in uno spazio M-dimensionale (dove M è spesso molto più grande di N). Esso entra in gioco nella ridefinizione della ditanza di R^2(x) per ogni punto x appartenente a X:
                [RISCRITTURA DI R^2 IN BASE A X]

            Questo parametro permette di modificare la forma della funzione di appartenenza. Prendendo lo stesso esempio e una funzione kernel gaussiana, vediamo in figura xxx come modificando sigma (parametro della funzione kernel gaussiana), viene modificata la forma della funzione di appartenenza.

                [FIGURA DI FUZZIFIER CON SIGMA MODIFICATO]

            Ricapitolando quindi, in possibilearn vediamo fondamentalmente 3 diversi iperparametri:
            
            * C: per determinare il peso degli errori di classificazione in fase di definizione del fuzzy set
            * fuzzifier: funzione che determina il grado di appartenenza all'insieme fuzzy
            * kernel: funzione utile a mappare gli elementi dallo spazio originale ad uno sovra-dimensionato



/*****************************************************************************************************************************************************************************************************

Capitolo 3: Esperimenti

In questo capitolo ci concentremo maggiormente sui risultati ottenuti tramite l'utilizzo di \emph{possibilearn} su due diversi dataset di volti: Olivetti faces e AT&T faces. Descriveremo inoltre tutti i risultati commentandoli e traendo infine delle conclusioni sull'algoritmo.

    Sezione 3.1: I dataset
        Sezione 3.1.1: Olivetti dataset
            Olivetti dataset è un dataset di immagini di volti umani. Le immagini sono foto scattate presso AT&T Laboratories Cambridge tra il 1992 e 1994. Questo dataset è stato messo a disposizione proprio da AT&T Laboratories Cambridge.
            Nel dataset sono presenti 10 immagini per 40 soggetti distinti, ognuna delle quali raffigura quest'ultimo in diverse condizioni di luce, con differenti espressioni facciali (occhi chiusi/aperti, sorriso, ecc.) per un totale di 400 immagini. Ogni immagine è stata quantizzata a 256 livelli di grigio e ha una risoluzione pari a 64 x 64. L'immagine è rappresentata da una lista di valori float (compresi tra 0 e 1) di 4096 elementi.

            [FIGURA DI HEAD DEL DATASET]

            Ogni soggetto è rappresentato tramite un numero che va da 0 a 39 che rappresenta l'etichetta di ogni record.

        Sezione 3.1.2: AT&T faces dataset
            Il dataset AT&T faces, come quello Olivetti, contiene anch'esso immagini raffiguranti 40 soggetti con 10 immagini distinte per ciascuno. Questo dataset, a differenza di Olivetti contiene immagini che sono grandi più del doppio di quelle raffigurate all'iterno del dataset di Olivetti. Ogni immagine a una risoluzione pari a 92 x 112, quindi con 10304 pixel. Ogni immagine ha 256 livelli di grigio e ritrae soggetti in diverse condizioni, esattamente come Olivetti. Ogni immagine è etichettata da un numero compreso tra 0 e 39 il quale rappresenta il soggetto.[nota: si ringrazia AT&T Laboratories per i dati messi a disposzione] 

            [FIGURA DI HEAD DEL DATASET?]
    
    Sezione 3.2: Valutazione del modello
        Fino ad ora abbiamo visto come si costruiscono i modelli di ML e come possono essere configurati. In questo capitolo ci soffermeremo sulla valutazione di un modello e i relativi iperparametri configurati.

        Sezione 3.2.1: Training, Validation e Test set
            Per costruire un modello abbiamo visto che è necessario un dataset su cui basare l'apprendimento. Il dataset che viene usato per la costruzione viene spesso diviso in due sotto-set: \emph{training set} e \emph{test set}.

            Il \emph{training set} è il sotto-set di dati utilizzata per allenare il modello. Quindi il modello vede e impara da questo set.

            Il \emph{test set} consiste nel set di dati del dataset escluso dal \emph{training set}. Questo set, è utilizzato per valutare l'effettiva accuratezza del modello. 
            Il \emph{test set} quindi, viene usato per determinare lo \emph{score} del modello nella generalità dei casi.
            % avente la migliore configurazione di iperparametri individuata mediante il \emph{validation set}. Lo score viene determinato analogamente a quanto viene fatto con il \emph{validation set}.

            A volte si definisce un terzo sotto-set denominato \emph{validation set}. È un set utilizzato per valutare le prestazioni con una specifica configurazione di iperparametri. Viene utilizzato quindi per una ricerca approfondita della migliore configurazione di questi. Una volta determinata la migliore configurazione del modello, questo viene nuovamente testato nel \emph{test set} per vedere come si comporta effettivamente dopo una più approfondita ricerca dei migliori iperparametri.
    
        Sezione 3.2.1: Cross-validation
            La \emph{cross-validation} è un metodo statistico di valutazione delle performance. Con la \emph{cross-validation} i dati dell'intero dataset vengono partizionati in $k$ gruppi ognuno dei quali è denominato \emph{fold} e dove $k$ rappresenta un numero specificato dall'utente. Quando viene utilizzata questa strategia con un $k$ pari a 5, significa che il dataset sarà diviso in 5 combinazioni differenti in 5 \emph{fold}. Per ogni combinazione possibile un \emph{fold} viene utilizzato come \emph{test set} e gli altri come \emph{training set}. Quindi il modello viene allenato per ogni combinazione data da questa strategia, utilizzando i $k-1$ fold per come \emph{training set} e il restante \emph{fold} come \emph{test set}. In Figura è mostrato uno schema di una \emph{cross-validation} a 5 fold.

            [FIGURA DEL LIBRO DELLA CROSS VALIDATION 5.1]

            L'utilizzo di questa strategia porta diversi vantaggi. Un primo importante vantaggio è rappresentato dal fatto che l'accuratezza del modello è calcolata sulla media di tutti i valori restituiti da ogni \emph{test set} nelle varie combinazioni dei \emph{fold}. Per capire meglio questo vantaggio, vediamo l'esempio seguente. Supponiamo di essere ``fortunati" e quindi di allenare il modello su un \emph{training set} che contiene solo dati di difficile classificazione. Questo significa che il \emph{test set} probabilmente avrà solamente dei dati molto facili da classificare il che permette al modello di avere prestazioni (non veritiere) molto alte. Al contrario se siamo ``sfortunati" avremo che tutti i dati di difficile classificazione sono all'interno del \emph{test set} e quelli semplici nel \emph{training set}. In questo modo avremo risultati piuttosto bassi.
            Con l'utilizzo di \emph{cross-validation} però, la possibilità di avere casi come questi sarà molto bassa proprio perchè verranno usati dati di allenamento sempre diversi per uno stesso modello. Il risultato finale sarà quindi composto dalla media di tutti i risultati delle varie combinazioni.

            Un'altro vantaggio corrisponde al fatto che oltre a variare l'utilizzo dei dati di allenamento, varia anche la dimensione di questi. Se utilizziamo 10 \emph{fold} ad esempio, significa che il \emph{test set} sarà definito da un decimo della dimensione totale del dataset. Analogamente se ne utilizziamo 5, il \emph{test set} sarò costituito da un quinto del totale.

            La \emph{cross-validation} presenta però un grosso svantaggio nelle performance. Questo perchè maggiore sarà il numero di \emph{fold} e maggiore sarà il tempo richiesto per ottenere un risultato. In altre parole con $k$ \emph{fold} avremo un tempo pari a $k$ volte il tempo necessario ad allenarlo.
        
        Sezione 3.2.3: Grid Search
            Ora che abbiamo capito come avviene e su cosa si basa una buona valutazione del modello possiamo passare al prossimo step: la ricerca del miglior modello.
            Cercare il miglior modello significa cercare la migliore configurazione di iperparametri che meglio generalizzano il problema originale. 
            \emhp{Grid Search} è una tecnica molto usata per la ricerca del miglior iperparametro. Questa tecnica consiste nel provare ogni possibile combinazione di iperparametri di nostro interesse. Consideriamo il caso di \emph{possibilearn}. Come abbiamo visto nel capitolo precedente vi sono tre differenti iperparametri che caratterizzano l'algoritmo: \emph{C}, \emph{kernel function}, \emph{fuzzifier}. Supponiamo di voler testare i valori 0.001, 0.1, 1, 1000 per C, un kernel gaussian con sigma pari a .225, .5, 1, 2 per il kernel e Linear fuzzifier, CrispFuzzifier per il fuzzifier. Così facendo abbiamo un totale di $ 4 \cdot 4 \codt 2 = 32 $ combinazioni differenti di iperparametri.
            Iterano tutte le possibili combinazioni su un \emph{training set} e valutandone le prestazioni su un \emph{test set} possiamo quindi trovare quale fra tutte le 32 combinazioni restituisca il valore più alto.
            
            Questa tecnica ha degli svantaggi perchè i risultati ottenuti non sempre sono del tutto veri. Questo succede perchè nel testare le prestazioni delle diverse configurazioni consideriamo \emph{test set} differenti per ognuna di queste. Così facendo i risultati ottenuti si basano tutti sul prorpio test set e questo non ci permette di avere un'unica misura per valutrare le effettive prestazioni.

            Per risolvere questo problema si ricorre all'utilizzo del \emph{validation set} che permette quindi di determinare quale possa essere la migliore configurazione e successivamente testarne le effettive prestazioni su dati che non sono stati usati per aggiustare la configurazione. In Figura zzz è mostrata la suddivisione del dataset con l'aggiunta del \emph{validation set}.

            [FIGURA DI DATASET IN TRAIN TEST E VALIDATION 5.5]

        Sezione 3.2.4: Grid Search con Cross-Validation e Nested Grid Search
            Nei due paragrafi precedenti abbiamo visto quale sia una buona valutazione delle performance del modello e successivamente di come si cerchi efffettivamente una buona configurazione di iperparametri. Queste due tecniche vengono spesso utilizzate insieme per vedere in generale quale sia la migliore combinazione possibile per un modello utilizzando diversi dati di \emph{training} e di \emph{test}.
            Quindi quello che viene fatto è, per ogni modello, provare su ogni combinazione di dati per il training e il test come funzione ogni combinazione di iperparametri. Questo consente, quindi, un ulteriore grado di valutazione del modello. Il grosso svantaggio di combinare queste due tecniche consiste nel tempo richiesto per la computazione. Per possibilearn C, sigma, fuzzifier come iperparametri configurabili. Supponendo di provare per C i seguenti valori:
                [LISTA DI IPERPARAMETRI DI C]
            , per sigma i valori: 
                [LISTA DI IPERPARAMETRI DI SIGMA]
            e mantendo il \emph{fuzzifier} fisso, otteniamo 5 x 5 = 25 possibili combinazioni mostrate in Figura xxx. 
                [TABELLA DI TUTTI LE COMBINAZIONI DEGLI IPERPARAMETRI]
            A questo le 25 configurazioni dobbiamo moltiplicalre per il numero di fold che usiamo nella cross validation. Prendendo, ad esempio, 5 fold otteniamo 25 x 5 = 125 diversi modelli da costruire!
            È importante quindi utilizzare il Grid Search unito alla Cross-Validation prestando attenzione a quali iperparametri si vogliono confrontare per evitare di non fare computazioni eccessivamente lunghe.
        
            **************PARLA DI NESTED CROSS VALIDATION**************
        
        Sezione 3.3: I risultati
            In conclusione analizziamo i risultati ottenuti sui dataset Olivetti e AT&T tramite possibilearn. Nella ricerca e nello studio di come si comporta possibilearn nel riconoscimento facciale e di classificazione dei soggetti sono stati usati tre computer per ridurre i costi temporali richiesti dall'allenamento di ogni modello. %La scelta di quale tecnica usare ha richiesto diversi giorni poichè il tempo richiesto dalla computazione era sostanzioso.
            
            Inizialmente è stata costruita un'infrastruttura client-server il cui server godeva di alte performance computazionali. Purtoppo queste non sono state sufficienti in quanto i tempi di calcolo erano troppo lunghi. Quindi la computazione è stata in seguito delegata a tecnologie di Google denominate Google Colaboratory le quali hanno di una maggiore performance computazionale permettendo il risparmio di risorse temporali.

            Per il calcolo dei risultati, a livello pratico, è stato usato \emph{python 3.7} come lignuaggio di programmazione il quale rappresenta uno dei più utilizzati linguaggi nell'ambito del ML per la sua comodità e semplicità di utilizzo e perchè sono presenti molte librerie ricche di funzionalità sviluppate proprio in \emph{python}. I pacchetti usati durante questi studi sono stati \emph{scitkit\_learn} per l'elaborazione dei dati e \emph{matplotlib} insieme a \emph{plotly} per la produzione di grafici 2D e 3D. In particolare sono state usate le seguenti librerie/funzionalità:
            \begin{itemize}
                \item{\emph{GridSearchCV}: funzione di \emph{scikit\_learn} che, presa una lista di valori per ogni iperparametro, applica su diverse suddivisioni del dataset la combinazione di ognuna di queste per trovare poi quella che definisce il modello con lo score medio più alto (come spiegate in \ref{sec:3.2}),}
                \item{\emph{pre-processing}: libreria di \emph{scikit\_learn} utile ad applicare delle trasformazioni ai dati prima che questi vengano usati per allenare il modello. In particolare \emph{StandardScaler} è una funzione di questa libreria per standardizzare le \emph{feature} e \emph{MinMaxScaler} per adattare a uno specifico intervallo (comune a tutte le \feature) i singoli valori delle \emph{feature},}
                \item{\empg{PCA}: funzione di \emph{scikit\_learn} che, come si può intendere dal nome, serve ad applicare una riduzione di dimensionalità tramite PCA}
                \item\emph{TSNE}: funzione di \emph{scikit\_learn} che, come si può intendere dal nome, serve ad applicare una riduzione di dimensionalità tramite TSNE}
                \item{\emph{pyplot}: libreria di \emph{matplotlib} che ha avuto un utilizzo speicifico nella produzione di grafici 2D quali grafici di dispersione (o \emph{scatter plot}), grafici di contorno e grafici a linea,}
                \item{\emph{express}: libreria di \emph{plotly} per la produzione di grafici tridimensionali come grafici di dispersione 3D e grafici di contorno 3D}
            \end{itemize}
            Inoltre è stato usato il pacchetto di \emph{fuzzylearn} (disponibile su github all'indirizzo https://github.com/dariomalchiodi/fuzzylearn) il quale contiene il codice per l'utilizzo di \emph{possibilearn}. Il modello codificato in questo pacchetto è stato adattato alle librerie di \emph{scikit\_learn} per poterne utilizzare le funzionalità di queste. Nello specifico l'adattamento richiede l'impkementazione delle seguenti funzionalità:
            \begin{itemize}
                \item{\emph{fit(X, y)}: la funzionalità principale di tutti i modelli di ML. \emph{fit} è utilizzata per allenare un modello sui dati \emph{X} conoscendone le rispettive etichette \emph{y},}
                \item{\emph{predict(X)}: contiene l'implementazione per predire la classificazione di ogni elemento di \emph{X}, dove \emph{X} è rappresentato da un vettore di elementi. Nel caso specifico di \emph{possibilearn}, \emph{predict} restituisce, per ogni $x$ di \emph{X}, la probabilità (tra 0 e 1) che $x$ appartenga al modello,}
                \item{\emph{score(X, y)}: contiene l'implementazione dell'accuratezza di predizione del vettore \emph{X} sapendo che le relative etichette sono \emph{y}. In particolare \emph{possibilearn} restituisce uno score pari alla differenza al quadrato della probabilità predetta dal modello e l'effettiva probabilità di appartenenza}
            \end{itemize}
            Nei prossimi due paragrafi vediamo i risultati ottenuti tramite l'utilizzo delle diverse tecniche di riduzione di dimensionalità PCA e t-SNE. Per ogni tecnica vediamo i risultati ottenuti sia per Olivetti dataset che per AT&T dataset. Inoltre, durante la descrizione dei risultati vediamo anche l'uso di alcuni degli strumenti sopra citati.

            Sezione 3.3.1: Utilizzo metodo di riduzione t-SNE
                In questo paragrafo ci concentriamo sui risultati otteuti applicando una riduzione tramite TSNE a 2 e 3 dimensioni. La scelta di queste dimensioni è dovuta al fatto che t-SNE permette di trasformare uno spazio a n dimensioni in uno spazio più piccolo mantenendo (e quindi focalizzandosi) le distanze tra i punti dello spazio originale. Per questo motivo è stato ritenuto più interessante ridurre le dimensioni originali in due dimensioni proiettabili in uno spazio naturale per farne poi un'analisi a livello qualitativo più immediata. Vediamo quindi la distribuzione dei punti quando vengono ridotti in due dimensioni.

                [SCATTER PLOT 2D OLIVETTI E AT&T]

                In Figura [SCATTER PLOT 2D OLIVETTI E AT&T] sono mostrati i grafici di dispersione per entrambi i dataset dopo aver applicato t-SNE a 2 dimensioni. Analizzando quindi i grafici è piuttosto immediata la distinzione dei vari gruppi rappresentanti ogni soggetto. Partendo da una considerazione del tutto qualitativa e visiva possiamo aspettarci come la costruzione dei modelli, con i giusti iperparametri, ci possa restituire una buona accuratezza nella classificazione dei soggetti.

                [SCATTER PLOT 3D OLIVETTI E AT&T]

                Diverso è, il caso riportato in Figura [SCATTER PLOT 3D OLIVETTI E AT&T] in cui sono mostrati i grafici di dispersione in 3 dimensioni. È facile notare come i cluster siano meno distinguibili rispetto alla proiezione in due dimensioni e quindi è naturale pensare che i risultati di classificazioni su 3 dimensioni siano inferiori rispetto a quelli su dati in 2 dimensioni. In Tabella [TABELLA TSNE OLIVETTI E AT&T] sono mostrati i rispettivi risultati: le colonne indicano i set su cui è stato testato l'insieme dei modelli mentre le righe indicano la dimensioni di riduzione raggiunte. 

                [TABELLA TSNE OLIVETTI E AT&T]

                Con la tabella è stato confermato quanto previsto nelle considerazioni visive fatte sui grafici di dispersione in Figura [SCATTER PLOT 2D OLIVETTI E AT&T] e in Figura [SCATTER PLOT 3D OLIVETTI E AT&T]. I risultati ottenuti sono frutto di 10 holdout ripetuti con una dimensione per il \emph{test set} pari al 20% del totale, di conseguenza il restante 80% è dedicato al \emph{training set}. 

            Sezione 3.3.2: Utilizzo metodo di riduzione PCA
                Ora vediamo come si comporta \emph{fuzzylearn} con i dati ridotti tramite PCA. In questo caso abbiamo considerato un numero maggiore di dimensioni rispetto a t-SNE perchè la logica che sta dietro a questo algoritmo è differente da t-SNE. Come abbiamo già visto nel Capitolo 1, PCA riduce le dimensioni eliminando le caratteristiche che contengono la minor quantità di informazioni. In Figura [PLOT LINE PCA RATIO] vediamo i grafici che mostrano come, la quantità di informazione, decresce al crescere di dimensioni da poter utilizzare. La scelta del numero di dimensioni è partita da un'analisi fatta su questo grafico il quale permette di capire fino a dove ha senso spingersi per cercare di ottenere il maggior numero di informazioni mantenendo la complessità del modello (e quindi il numero di caratteristiche che descrivono ogni osservazione) più bassa possibile.
                Con PCA sono state sperimentate le riduzioni a 2, 5, 20, e 50 dimensioni. L'unica riduzione che è possibile analizzare è, naturalmente, quella a due dimensioni. Pertanto in Figura [SCATTER PLOT PCA OLIVETTI E AT&T] è possibile vedere la distribuzione delle osservazioni nel piano. A differenza di t-SNE, in questi grafici si riesce a distinguere solo qualche piccolo cluster e, oltretutto, facendo fatica. Questo non è un buon segno, infatti come vediamo nei risultati più avanti l'accuratezza di classificazione data dall'insieme dei 40 modelli è decisamente di scarsa qualità. Uno dei motivi per cui la differenza è così marcata tra t-SNE e PCA sta proprio nel fatto che le due tecniche affrontano lo stesso problema, ovvero quello di ridurre le dimensioni, analizzando aspetti totalmente diversi. Mentre t-SNE confronta le distanze tra i punti in un iperspazio, PCA va a selezionare solo le \emph{feature} che mantengono la maggior quantità di informazioni. Ritornando al grafico in Figura [PLOT LINE PCA RATIO] vediamo che utilizzando le prime due dimensioni otteniamo solamente il 30% per Olivetti e il 38% per AT&T. Questo dato rispecchia piuttosto bene la distribuzione del grafico di dispersione proprio perchè con solamente il 30% e il 38% vi è un'alta probabilità che i dati siano mischiati. Quindi vediamo in Tabella [TABELLA PCA OLIVETTI E AT&T] i risultati ottenuti nei due dataset tramite l'utilizzo di PCA.

                [TABELLA PCA OLIVETTI E AT&T]

                Nella tabella sopra riportata è infatti possibile notare un certo andamento dei risultati che è direttamente proporzionale al numero di feature utilizzate. Questo segue esattamente quanto scritto sopra: con l'aumentare delle dimensioni aumentano le caratteristiche che descrivono l'oggetto.
            
            Sezione 3.3.3: Analisi conclusiva
                Nell Tabelle [TABELLA TSNE OLIVETTI E AT&T] e [TABELLA PCA OLIVETTI E AT&T] abbiamo visto quale sia l'accuratezza fornita da \emph{fuzzylearn} sui dataset Olivetti e AT&T mediante le due tenciche di riduzione PCA e t-SNE. Da questi risultati si evince che entrambe le tecniche portano a buoni e interessanti risultati con la differenza che gli spazi da considerare sono molto diversi. Con t-SNE abbiamo potuto osservare la sua potenzialità nell'immediatezza visiva di rappresentazione dei dati, il quale è un ottimo strumento per poter fare analisi qualitative dei dati. Con PCA, invece, è stato possibile vedere come la ``qualità" dei dati influsica sull'accuratezza dei modelli allenati su di essi. Infine possiamo dire che \emph{fuzzylearn} è un ottimo algoritmo che permette, nel caso specifico di riconoscimento facciale, buone prestazioni a detrimento del tempo richiesto per allenare i modelli. Gli aspetti positivi ricadono, quindi, nella possibilità di rendere i modelli molto ``malleabili" grazie ai diversi iperparametri di cui gode e quindi di adattarsi bene a contesti complessi come il riconoscimento di volti in immagini, mentre lo svantaggio principale consistono nel tempo richiesto per allenare il singolo modello.

            Sezione 3.3.1: Olivetti dataset
                Analizziamo ora i risultati ottenuti sull'analisi di Olivetti dataset. In primo luogo analizziamo i dati di questo dataset. Essi sono suddivisi in due diversi vettori di diverse dimensioni. Il primo vettore di cui parliamo è un vettore bidimensionale contenente 400 vettori di dimensione 64x64 = 4096. Ogni vettore della matrice rappresenta le immagini sottoforma di valori float compresi tra 0 e 1. Il secondo vettore monodimensionale rappresenta, invece, le etichette relative ad ogni immagine sottoforma di interi compresi tra 0 e 39 (inclusi). Il vettore dell'immagine i-esima avrà l'etichetta contenuta nell'i-esimo elemento del vettore delle etichette.

                La prima analisi è stata fatta sul numero di dimensioni. che sono eccessivamente elevate. Considerare 4096 dimensioni non sarebbe utile, in quanto, come spiegato nel Capitolo 1, trattandosi di dati che rappresentano immagini, molti dati possono essere esclusi perchè poco significativi. Per questo motivo, tramite le tecniche di riduzione della dimensionalità, sono state ridotte le dimensioni a 2 e a 5 nel caso di PCA e a 2 e 3 nel caso di t-SNE[1].
                Partendo dalla riduzione a 2 dimensioni di t-SNE è possibile osservare, tramite lo scatter plot in Figura fig:scatter_plot_tsne_2_of, la distribuzione dei dati. Nella figura è possibile osservare come vengano suddivisi piuttosto bene i punti, questo, infatti, ci può far pensare a delle buone classificazioni.

                [SCATTER PLOT PER TSNE 2D OLIVETTI]fig:scatter_plot_tsne_2d
                [SCATTER PLOT PER TSNE 3D OLIVETTI]fig:scatter_plot_tsne_3d

                Nel caso di una riduzione a 3 dimensioni in Figura fig:scatter_plot_tsne_3_of è possibile osservare come i dati non seguino più (rispetto al caso a 2 dimensioni) una suddivisione distinta dei dati. Questo, al contrario del caso a 2 dimensioni, ci fa sospettare che i risultati in caso di test non saranno molto alti. In Tabella tab:of_tsne abbiamo un confronto tra le precisioni in base all'uso di un diverso numero di dimensioni.

                [TABELLA CON TSNE COMPONENTI TEST-TRAIN]tab:of_tsne

                Nel caso di PCA in Figura fig:scatter_plot_pca_2_of osserviamo una grossa differenza rispetto alle due dimensioni date da t-SNE. In questa figura, infatti, è possibile notare come i dati siano poco distinguibili. Questa bassa suddivisione, analogamente alla suddivisone a 3 dimensioni di t-SNE non fa aspettare buoni risultati. Per quanto riguarda i dati ridotti a 5 dimensioni non è possibile effettuare un'analisi qualitativa per ovvi motivi. Quindi ci siamo limitati a cercare tramite una Grid Search con Cross Validation degli iperparametri che potessero rendere performanti i modelli di ogni soggetto.

                [SCATTER PLOT PER PCA 2D OLIVETTI]

                Infine in Tabella tab:pca_of sono stati riportati i risultati per PCA. Per il caso a 2 dimensioni, come potevamo aspettarci abbiamo ottenuto scarsi risultati proprio per il problema che presentano gli stessi dati nella suddivisione. Nel caso di 5 dimensioni, invece, abbiamo ottenuto un discreto risultato che rappresenta una classificazione dei soggetti corretta nel 70% in caso di dati di \emph{test} e del 90% in caso di dati di \emph{train}. 

                [TABELLA CON PCA COMPONENTI TEST-TRAIN]tab:pca_of

            Sezione 3.3.1: AT&T dataset

                Analogamente a come fatto per il dataset di Olivetti, procediamo con l'analisi del dataset di AT&T. In questo dataset i dati sono della stesso tipo del dataset precedente, la differenza consiste in particolare nella quantità di dati a disposizione. Le immagini di questo dataset, anch'esse rappresentate da dei vettori di numeri \emph{float} compresi tra 0 e 1 di lunghezza pari al numero di pixel di ogni immagine. In questo caso le immagini sono costituite da 92x112 pixel per un totale di 10304 elementi per vettore.
                Questo dataset è stato analizzato in parallelo al dataset visto precedentemente per valutare le prestazioni di \emph{possibilearn} in casi di immagini di dimensioni maggiori. In questo caso i dati sono stati ridotti in primo luogo a 2 e 3 dimensioni con t-SNE per avere una prima idea a livello grafico. Parallelamente è stato analizzato con PCA a 2, 5 e 20 dimensioni.
                Seguendo il procedimento utilizzato per il datatset di Olivetti è stata prima fatta un'analisi qualitativa dei dati (quando possibile) e successivamente una ricerca dei migliori iperparametri.
                In Figura fig:sccatter_plot_at_tsne_2 è riportato il caso di t-SNE a 2 dimensioni e in Tabella tab:at_tsne i relativi risultati ottenuti. Analogamente è stato fatto per t-SNE a 3 dimensioni.

                [SCATTER PLOT PER TSNE 2D ATET]fig:sccatter_plot_at_tsne_2
                [SCATTER PLOT PER TSNE 3D ATET]fig:sccatter_plot_at_tsne_3

                In Figura fig:scatter_plot_at_pca_2 è mostrata la distribuzione dei dati nel caso di PCA e in Tabella tab:at_pca i risultati utilizzando diverse riduzioni di dimensioni tramite PCA.
                [SCATTER PLOT PER PCA 2D ATET]fig:scatter_plot_at_pca_2

                [TABELLA CON PCA COMPONENTI TEST-TRAIN]{tab:pca_of}

            


                
[1: Nel caso di t-SNE sono state considerate solo le riduzioni a 2 e a 3 dimensioni perchè essendo che t-SNE non toglie informazioni (come fa PCA), utilizziamo 2 e 3 dimensioni perchè in questo modo siamo in grado di plottare i dati e vedere se sono indivudabili dei cluster per ogni soggetto]    
[nota A: Per ogni Grid Search che forniscono 6 combinazioni con Cross Validation a 3 fold richiedevano per modello 24 minuti, quindi un totale di 16 ore per allenare tutti i modelli]
/*****************************************************************************************************************************************************************************************************
            This is an “unsolved” problem in statistics: there are no
magic procedures to get you the “best model.”
■ In some sense, model selection is “data mining.”
■ Data miners / machine learners often work with very many
predictors.

To “implement” this, we need:
◆ a criterion or benchmark to compare two models.
◆ a search strategy.
■ With a limited number of predictors, it is possible to search
all possible models.
            tra una serie di parametri (anche chiamati iperparametri), la migliore combinazione per costruire un modello di Machine Learning 

            Una volta definito il confine che delimita la sfera, bisogna definrie il confine oltre il quale il grado di appartenenza 3rò pari a 0. Per farlo dobbi



        In particolare tutti i punti x aventi membership pari a 1, soddisfano la regola:
            la distanza di x da alfa (nella soluzione ottimale) <= R^2(xi) dove gli xi rappresetano i support vector.
        
        
        
        
        
        Thus the corresponding x i has an
image through Φ lying on the border of the learnt sphere S and will be called
support vector.
        In particular, all points x with membership μ A (x) = 1 satisfy R 2 (x) ≤ R 1 2 ,
where R 1 2 = R 2 (x i ) for any support vector x i .
In particolare tutti i punti x aventi membership pari a 1, soddisfano al regola:
    la distanza di x da alfa (nella soluzione ottimale) <= R^2(xi) dove gli xi rappresetano i support vector.
         risolviamo il problema ottenendo le variabili con i risultati ottimali.

/*********************************************************************/
            Sezione 2.3.2: Hyper-paramter tuning 

             

            Nella maggior parte dei modelli di Machine Learning, oltre a fornire i dati su cui allenarsi, bisogna definire una serie di _iperparametri_[nota: differenza tra iperparametro e parametro. Si chiamano iperparametri e non parametri perchè questi indicano due cose differenti. Gli iperparametri sono dei parametri configurati dall'utente, mentre i parametri sono valori generati automaticamente dalla macchina] che influenzano la costruzione dei modelli. Ogni modello necessita i propri iperparamtri. Per capire meglio di quali dati parliamo, facciamo un esempio. Prendendo l'algoritmo k-Nearest Neighbor, come abbiamo già visto, bisogna determinare la k. La k, in questo caso, rappresenta un iperparametro del modello costruito con k-NN. La scelta degli iperparametri è importante, perchè tramite questi si influenza la predizione fatta dal modello.
            Inoltre, è giusto sottolineare che non esistono dei buoni o dei cattivi iperparametri, bensì questi vengono definiti buoni per i dati di allenamento. Questo significa che ci sono dei casi in cui, ad esempio, il k-NN si comporta bene con un k pari a 1 e altri casi in cui k è pari a 9.
            
            Con l'hyper tuning optimization, di fatto, viene cercata la migliore combinazione possibile di iperparametri per il modello che si sta allenando. Questo può essere fatto, oltre che manualmente, nei modi automatici di seguito indicati:

            * Random search 
            * Pipeline
            * Gridsearch

            Con tecnica manuale si intende una ricerca di iperparametri che l'utente fa basandosi su intuizioni e/o esperienza pregressa.
            GridSearch prevede che l'utente fornisca una serie di valori per ogni iperparametro. Viene calcolata e utilizzata ogni possibile combinazione di iperparametri per generare un modello. Inoltre, GridSearch è la tecnica che è stata utilizzata per l'ottimizzazione degli iperparametri all'itenrno del progetto di questo elaborato, pertanto lo vederemo in dettaglio nel prossimo capitolo. 
            Random search prevede che l'utente fornisca una serie di valori per ogni iperparametro. Questa tecnica cerca tramite combinazioni casuali di iperparametri, il modello che restituisca il risultato più alto. Diversamente dalla tecnica
            La tecnica Pipeline è adibita ad una ricerca tra diversi algoritmi aventi diverse possibili combinazioni di iperparametri.

In questo algoritmo le etichette da assegnare ai singoli dati corrispondono a dei gradi di appartenenza ad un insieme. Con esse l'algoritmo è in grado di

Questo algoritmo va considerato come un estensione dell'SVC (Support Vector Cluster).
Questo algoritmo, come l'SVC, sfrutta i vettori di supporto per determinare la zone di clustering
Questo algoritmo va a modificare il comportamento del SVC (Support Vector Cluster). Il Support Vector Cluster è un algoritmo utile a classificare in diversi cluster i dati in uno spazio. Analogamente all'SVC in questo algoritmo i dati si suddividono in gruppi, con la differenza che essi non hanno una semplice bivalenza (compresi e non), bensì si basano secondo la logica degli insiemi fuzzy.


Quindi per questo algoritmo è più corretto parlare di regressore, in quanto il risultato è dato dal grado di appartenenza ad un insieme. 


/*********************************************************************************************************INTRODUZIONE

Fornire al lettore le indicazioni che gli peremttono di capire in che limit è circoscritto l'indagine della tua tessi
qual'è l'obiettivo
quali sono gli strumenti (di calcolo di pensiero)

spiageare come è suddiviso il libro e per quale motivo

parlarew del riconoscimento facciale e della logica fuzzy

/************************************************************************************************************CONCLUSIONE

difficiolta della ricerca e della tesi in generale