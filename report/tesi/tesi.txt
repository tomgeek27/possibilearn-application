Capitolo 1: L'apprendimento automatico

    TODO: Cos'è l'apprendimento automatico?

    Sezione 1.1: Approcci

    I tipi di algoritmi di machine learning differiscono nel loro approccio, nel tipo di dati che inseriscono e producono e nel tipo di attività o problema che devono risolvere. Il machine learning può generalmente essere suddiviso in due macro categorie: supervisionato, non supervisionato. A queste viene spesso aggiunta anche una terza che si chiama "apprendimento con rinforzo".

        Sezione 1.1.1: Supervisionato

        L'approccio supervisionato è una tecnica che prevede di lavorare su un insieme di dati etichetti dall'utente, da cui possa imparare a riconoscerne le varie differenze e in seguito a fare una predizione sulla possibile etichetta da attribuire. Questa tecnica può fornire due diversi tipi di risultati: discreti o continui. Per comprendere meglio questo concetto proviamo a fare un esempio.

        Parliamo di diagnosi mediche di una serie di pazienti. Quindi analizzando le diagnosi, un medico è in grado di definire se il paziente è in salute o meno. Da qui possiamo estrapolare quindi due insiemi/etichette differenti per il nostro caso: "in salute" e "malato". Fornendo come input ad un classificatore questo insieme di dati con le rispettive etichette appena definite, la macchina, tramite l'algoritmo, sarà in grado di fornire delle predizioni sulla possibile etichetta da attribuire ad ogni nuova diagnosi. Alle prime iterazioni vi è un'alta probabilità che l'errore nella predizione sia alta (a causa, ad esempio, di outlier, i quali rischiano di "confondere" l'algoritmo durante l'apprendimento). Proprio per questo è necessario fare diverse iterazioni, in modo che l'algoritmo capisca dove ha sbagliato (andando a confrontare la predizione con l'etichetta effettiva) e "aggiusta", di conseguenza, la predizione. Viene quindi naturale pensare che maggiore sia la mole di dati (le diagnosi mediche nel nostro caso), maggiore sarà il numero di casi in cui si può etichettare un elemento con maggiore precisione, proprio perchè impara da tutti i possibili casi presentatigli.
        In questo esempio abbiamo utilizzato solamente le classi "in salute" e "malato", ma nulla ci vieta di definirne una terza o una quarta, ad esempio possiamo etichettare un paziente come "in salute", "malato" e "malato terminale".

        Nel caso in cui, non vogliamo avere solo una classificazione della salute del paziente, ma preferiamo quantificare l'aspettativa di vita del paziente, non è più possibile ricorrere a dei classificatori. Da qui nasce la necessità di passare da un valore discreto ad un valore continuo: i regressori. 
        I regressori servono nel momento in cui si vuole quantificare un certo oggetto. Riprendendo l'esempio precedente, potremmo voler quantificare i tempi di guarigione di un malato, data una specifica diagnosi.
        
        Nei modelli di machine learning supervisionati vogliamo che questi si comportino bene con dati nuovi, mai visti prima e sui quali venga fatta una predizione il più precisa possibile. Per assicurarci di questo, dobbiamo assicurarci che il modello stiano lontano dall'overfitting (sovra-adattamento) e dell'underfitting (sotto-adattamento), vediamoli in dettaglio.
        L'overfitting consiste nell'adattare il modello in maniera eccessiva ai dati che gli sono stati forniti per allenarsi, il che non permetterebbe di generalizzare bene il modello per i nuovi dati mai visti prima perchè anche con un piccolo scostamento da quelli che sono i vincoli che determinano la predizione, comporterebbe una predizione sbagliata.
        L'underfitting invece, consiste in tutto il contrario dell'overfitting, ovvero nell'allenare il modello su delle regole troppo semplici e poco robuste, il che comporterebbe un modello che effettua delle predizioni su regole troppo vaghe.
        
/*        la funzione generalizzi l'insieme dei punti mantendo una soglia di errore non eccessiva. Questo perchè se la funzione approssima in maniera troppo precisa (overfitting) all'insieme dei punti, si rischia che i futuri elementi che verranno analizzati vengano predetti nel modo sbagliato, proprio perchè la funzione si è adattata in maniera eccessiva al campione inziale su cui l'alogirmto si è allenato. Se invece l'errore è eccessivo (underfitting) singificherebbe che la funzione non si avvicina nemmeno ai dati iniziali, quindi, anche qui, i futuri elementi da predire saranno errati.*/

        [TABELLA DI DATA CUSTOMER t1.0]

        Per un esempio più concreto su quali siano i problemi generabili dall'overfitting e l'underfitting vediamo la tabella [t1.0] dove si hanno dei dati in merito a delle persone. Supponiamo di voler predire se il cliente X vorrà acquistare una barca. Analizzando la tabella piuttosto attentamente si può notare che secondo la regola

        "Se un cliente ha meno di 45 anni, ha meno di 3 figli o non è divorziato, allora lui vorrà comprare una barca",

        secondo la quale, le predizioni (su questo dataset) saranno giuste al 100%! Ma questo singificherebbe anche che se in futuro un cliente C che volesse comprare la barca non rispettasse la regola (magari perchè ha semplicemente 46 anni o perchè ha 4 figli), la predizione del sistema sarebbe "C non vuole comprare una barca", il che sarebbe quindi errato. Questo è il caso dell'overfitting: stabilire le regole di predizione su troppi dati, in maniera troppo rigida.
        
        [IMMAGINE DI OVERFITTING] (1.1)

        Lo stesso si può fare al contrario, ossia quando le regole di predizione adottate dal sistema si basano su troppi pochi dati e sono troppo vaghe: facciamo un esempio.
        Supponiamo che il sistema identifichi un cliente come possibile acquirente di una barca se segue la seguente regola:

        "Se un cliente ha una casa allora vorrà comprare una barca"

        è naturale, leggendo la regola, pensare che questa sia sbagliata. Questo è il caso dell'underfitting, ossia il caso in cui si definiscono un modello che segue regola poco solide e troppo semplici.

        [IMMAGINE DI UNDERFITTING] (1.2)

        Quindi se il modello è sovra-adatttato al dataset di allenamento oppure sotto-adattato, si rischia di avere una algoritmo di scarso rendimento. Tuttavia esiste un hotspot che si trova a metà tra l'overfitting e l'underfitting, il quale permette di ottenere il miglior rendimento nella totale generalità dei casi. Questo è quello che si vuole ottenere da un algoritmo di apprendimento automatico.

        Il trade-off tra l'overfitting e l'underfitting è illustrato in figura 1.3

        [IMMAGINE DI TRADE-OFF 1.3]

            *** Sezione 1.1.1.1: k-Nearest Neighbor

            Vediamo nello specifico uno dei più semplici algoritmi di machine learning: k-Nearest Neighbor. Questo è un algoritmo utilizzato sia per la classificazione che per la regressione. In entrambi i casi l'algoritmo si basa sul un parametro fissato k il quale indica il numero di vicini da considerare.

            Supponiamo di avere due feature (per semplcità), 'a e 'b le quale descriveranno - insieme all'etichetta - ogni record del nostro dataset. 

            Nel caso di classificazione tramite il k-NN, un nuovo elemento non ancora etichettato, (il quale avrà le proprie coordinate ('a, 'b)), verrà classificato in base al tipo predominante dei suoi vicini. La scelta del k, è quindi l'unica, ma fondamentale scelta per determinare la precisione nella predizione dei futuri elementi da classificare.

            In figura xxx viene mostrato come influsice la scelta di differenti parametri k su uno stesso campione C.

            [IMMAGINE DI KNN SU CAMPIONE C CON K = 1]
            [IMMAGINE DI KNN SU CAMPIONE C CON K = 5]
            [IMMAGINE DI KNN SU CAMPIONE C CON K = 9]

            Questo algoritmo è utilizzato con un k dispari, il quale non permette di avere casi di indecisione e di poter sempre definire a quale classe appartiene.
            
            [IMMAGINE DI K-NN CLASSIFIER 1.4]

            Nel caso di regressione tramite il k-NN, il risultato sarà pari alla media del valore target che vogliamo predire di tutti i k vicini. Vediamo un esempio in dettaglio del funzionamento.

            Immaginiamo che nel nostro dataset abbiamo due caratteristiche 'a e 'b per ogni elemento, dove 'a è il valore su cui vogliamo basare il modello e 'b è il valore che siamo interessati a predire. Prendendo ad esempio, un k pari a 3 significherà (come vediamo in figura 1.5) che il nuovo elemento 'elem avrà come valore target la media dei target dei 3 elementi più vicini, sull'asse delle ascisse (ovvero l'asse su cui sono disposti i 'a)

            [IMMAGINE DI K-NN REGRESSOR 1.5]

            *** Sezione 1.1.1.2: Linear models

            I modelli lineari sono una classe di modelli che cercano di effettuare predizioni basandosi su una funzione lineare basata sull'insieme delle feature o caratteristiche appartenenti all'elemento da analizzare. 
            Nel caso della regressione, la funzione di cui parliamo è definibile come segue:

                y' = w[0] * x[0] + w[1] * x[1] + ... + w[n] * x[n] + b

            dove n rappresenta il numero di feature, x le feature, w i pesi da attribuire alle singole feature e la b rappresenta i parametri attribuiti al modello che si sta allenando.
            Prendendo una sola feature (quindi n pari a 1), y' risulterebbe:

                y' = w[0] * x[0] + b
            
            la quale è esattamente una funzione di una linea retta, dove w è l'angolo e b è lo scostamento dall'origine degli assi.

            [IMMAGINE DI UNA REGRESSIONE LINEARE OER REGRESSIONE] (1.0)

            Per riprendere l'esempio precedente, supponiamo che si voglia quantificare il numero di giorni necessari per guarire un paziente malato.
            Supponiamo, per semplcità, di avere una sola caratterisitica definita all'interno della diagnosi che rappresenta l'età del paziente (sull'asse delle ascisse) e per ogni punto il relativo tempo di guarigione - in giorni - (sull'asse delle ordinate). 
            Avendo quindi una serie di punti, è possibile tracciare una retta che approssima a tutti i punti definiti nel campione di allenamento (anche chiamato training set).
            Nell'immagino 1.0 è illustrata la retta appena citata.

            Tutto questo, per quanto riguarda l'utilizzo di modelli lineari nella regressione, nella classificazione invece si mantiene la stessa formula con la piccola differenza che si introducono degli intervalli per definire a quale classe appartiene il singolo caso. Nella classificazione binaria, ad esempio la formula risulterebbe come segue:

                y' = w[0] * x[0] + w[1] * x[1] + ... + w[n] * x[n] + b > 0
            
            dove, supponendo di avere le classi "1" e "0", se la y' fosse maggiore di 0 appartiene alla classe "1", alla classe "0" altrimenti.

            TODO: aggiungere la suddivisione in multiclassi
            
            [IMMAGINE DI REGRESSIONE LINEARE PER CLASSIFICAZIONE]

            Nella figura xxx è mostrato l'esempio appena citato, dove i cerchi sono la classe 0 e i triangoli la classe 1.

            *** Sezione 1.1.1.3: Support vector Classifier

            I SVC (Support Vector Classifier) sono una classe di modelli che si preoccupa di individuare un iperpiano utile a separare (e quindi classificare) i punti nel piano in diversi gruppi, i quali vanno a definire le classi che ci permettono, una volta inseriti i nuovi dati, di attribuire a questi, la rispettiva classe.

            Il primo problema che i SVC devono risolvere, è capire quale sia l'iperpiano che suddivide nel modo migliore i dati, questo perchè dati, ad esempio, due gruppi di punti è possibile dividerlo in infiniti modi. 
            Una volta presi gli iperpiani candidati, per decidere qual'è il migliore, si prendono i punti P più vicini a questo iperpiano I, i quali vengono definiti vettori di supporto. Per ogni retta R e i relativi vettori di supporto V, viene calcolata la distanza tra R e V la quale indicherà il "margine". Si definisce retta migliore, la retta che riesce a massimizzare il proprio margine dai vettori di supporto. In figura xxx è possibile vedere tutti i componenti dell'SVC in gioco.

            Tutto questo però accade quando abbiamo solamente due dimensioni. Quando ci confrontiamo con dei problemi reali, le feature in gioco che definiscono i punti sono molte, molte di più. Per ovviare a questo problema si ricorre alle funzioni kernel, le quali sono delle funzioni in grado di mappare dei vettori definito in un spazio a n dimensioni, in uno spazio a m dimensioni.
            Questo 'trick' del kernel consente di riutilizzare i SVC anche laddove non è possibile suddividere i punti tramite una semplice retta.

            Supponiamo di avere un caso mostrato in figura xxx, dove si hanno due sole dimensioni, ma dove non è possibile suddividere i punti con una semplice linea.


            Tramite una funzione kernel trasformiamo questi due punti definiti dalle coordinate x e y, in dei punti definiti dalle coordinate x, y e z, in questo modo possiamo rivedere il tutto in 3 dimensioni e tracciare un iperpiano che suddivide correttamente i punti nello spazio a 3 dimensioni, per poi successivamente ridefinirlo secondo le due dimensioni iniziali di partenza.

            Come esplicato in un articolo di towards data science (https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989), riporto l'esempio esplicativo:
            
            Presi dei punti in uno spazio bidimensionale, ci capita la situazione in cui questi non sono suddivisibili tramite una retta.

            [IMMAGINE DEI PUNTI DISPOSTI IN MANIERA NON DIVISIBILE IN UN PIANO A 2 DIMENSIONI]

            Aggiungiamo una terza dimensione z e definiamola come segue:

                z = x**2 + y**2

            Il risultato ottenuto è dato dalla seguente immagine:

                [IMMAGINE DEI DATI PLOTTATI SULL'ASSE Z]
            
            Notiamo subito che ora è facile suddividere i punti in due distinti gruppi tramite una retta k (che rispetta il principio delle SVM):

                k = z
            
            dato che z è stato definito come:

                z = x**2 + y**2
            
            poniamo la retta k come segue:

                k = x**2 + y**2
            
            il che ci consente di ottenere una linea nello spazio a due dimensioni, che sarà esattamente la nostra divisione tra i gruppi.

            Quindi quando ci si trova davanti a problemi n-dimensionali bisogna sempre ricorrere al trucco del kernel. Questo trucco prevede anche dei parametri C e Gamma, detti parametri di tuning.
            Essi influiscono sulla selezione dell'iperpiano che si va ad ottenere nello spazio inziale:
            *C: è il parametro che consente di definire un costo nell'errore della suddivisione dei punti, vale a dire che nel caso in cui si scelga una C grande significa che ogni singolo errore avrà un costo elevato, andando cosi ad adattare il modello SVM il più preciso possibile al training set. Adattando il modello in maniera eccessiva al training set, però, si rischierebbe di andare in overfitting e quindi comporterebbe ad una classificazione errata nel caso di esempi estranei all'insieme dei dati di allenamento. Avendo invece, una C piccola, significa che il costo di errore sarà basso, il quale comporterà che durante l'allenamento del modello, saranno presenti diversi errori di classificazione, però, così facendo si sta generalizzando maggiormente il modello a nuovi casi da etichettare, il che può essere positivo. Come citato sopra, vi rimando al grafico che denota che lo sweetspot, ossia il compromesso per ottenere un buon risultato di predizione, è a metà tra l'underiftting e l'overfitting.

            *Gamma: TODO

            *** Sezione 1.1.1.5: Alberi di decisione

            Gli alberi di decisione o decision trees, è un algoritmo di classificazione e regressione, che, fondamentalmente, basa la sua logica sull'apprendimento di una struttura, sulla domanda 'se questo allora...'/'altrimenti questo...' fino ad arrivare ad una decisione finale.

            Prendo spunto da un esempio fatto nel libro 'Introduction to machine learning' in cui si vuole distinguere un animale tra: falco, pinguino, delfino, orso.
            L'algoritmo lavora andando a fare determinate domande a cui puoi rispondere vero o falso, ad esempio, si parte da una domanda che può semplicemente essere: "Ha le piume?". In questo modo si aprono due strade, "ha le piume" e "non ha le piume", suddividendo in due gruppi distinti gli animali. Prendendo gli animali che non hanno le piume (orso e delfino), e facendo un'ulteriore domanda, che differenzia i due animali, si può arrivare a capire di quale animale si sta parlando tramite questa serie di risposte. Seguendo la domanda, "Ha le pinne?", possiamo capire che se la risposta è si, è il delfino, ovvero l'unico animale tra i 4 sopra scelti che non ha le piume e ha le pinne.

            Seguendo questa logica è possibile arrivare (con le giuste domande) a dire di quale tipo di animale si sta parlando.

            Questo era un semplice esempio che non rispecchia la realtà, e i campi di utilizzo di questo algoritmo. Spesso i dati che vengono analizzati hanno dei valori di tipo continuo, quindi la domanda a cui si risponde, tendenzialmente, è del tipo: "x è maggiore di y?". Questo è il modo in cui operano questi algoritmi in caso di valori di tipo continuo e così facendo si va a costruire un completo albero di decisione.

            Come abbiamo già visto anche negli altri algoritmi, il problema dell'overfitting e underfitting è un problema ricorrente e nel caso dell'albero di decisione non è da meno.
            Infatti se viene costruito un albero troppo dettagliato e quindi con un livello di profondità eccessivo si va ad adattare in maniera eccessiva (quindi in overfitting) al training set, andando a definire zone anche molto piccole, come viene mostrato in figura xxx.

            [IMMAGINE DECISIONN TREE OVERFITTING]

            Analizzando un caso simile è possibile vedere in figura xxx come l'errore su un test set (quindi un insieme di dati utile a testare il modello allenato), cresca rispetto al train, questo succede proprio perchè il modello non è generalizzato, ma si è allenato specificandosi in maniera eccessiva al training set.

            [IMMAGINE GRAFICO ERRORE SU TEST SET CON OVERFITTING]

            Per risolvere questo problema, esistono due strategie:
            * far terminare lo sviluppo dell'albero dopo pochi passi (pre-potatura): limitare la profondità dell'albero definendo una variabile che tenga conto della prodfondità dell'albero, la quale quando supera una soglia prefissata, interrompe lo sviluppo di questo
            * rimuovendo i nodi che hanno contengono informazioni ridotte (post-potatura)

        Sezione 1.1.2: Unsupervised

        Il secondo importante approccio all'applicazione del machine learning, consiste nella tecnica non supervisionata. Cosa si intende dire con non supervisionata? Come può una macchina imparare se nessuno la guida nella scelta di decisioni?
        Questa è proprio la sfida che si vuole superare con questa tecnica, ovvero far estrapolare alla macchina, delle informazioni "nascoste" all'interno dei dati che gli vengono forniti. Cercando dei legami o regole che i dati tendono a seguire, all'interno del set datogli in input senza fornire nessun tipo di ifnormazione ulteriore che possa guidare il modello di apprendimento in qualche modo (a differenza, appunto, del modello supervisionato).

        Ci sono diversi tipi di machine learning non supervisionato, in questo capitolo ci limiteremo a elencarne la logica che seguono e quale possibile campo di utilizzo, senza entrare troppo nello specifico.

        Un primo tipo di Unsupervised learning è il clustering. Il clustering è una tecnica che serve per suddividere in gruppi distinti degli elementi che hanno dati e caratteristiche in comune. Per essere più chiari, vediamo subito un possibile esempio.
        Supponiamo di aver scattato una serie di foto i cui soggetti sono delle persone, magari nostri amici o parenti e decidiamo di caricarle su un social network. Durante il caricamento, il social su cui le stiamo caricando, si permette di visionare le foto e applicare proprio un algoritmo di clustering. In che modo? L'algoritmo non sa nè chi siano le persone raffigurate nè quante esse siano. L'algoritmo andrà a cercare tutti i volti nelle foto che abbiamo caricato e successivamente, dopo aver ottenuto una lista di tutti i volti estrapolati da ogni foto, tramite un algoritmo di clustering va a cercare somiglianze in questi volti, andando così a raggruppare le foto dove è presente lo stesso soggetto.

        ***TODO***: ulteriori esempi

       ***  Sezione 1.1.3: Semi-supervised

        L'approccio semi-supervised, non è un vero e proprio approccio, bensì una tecnica che sta a metà tra le due appena viste (supervisionato e non supervisionato). Questa tecnica consiste nel combinare le due tecniche e quindi fornire un risultato basandosi su un input eterogeneo: etichettato e non.

        Questo approccio risulta utile quando si ha una grande mole di dati, e gli utenti che sono in grado di etichettare i dati sono utenti specializzati, quindi, nella situazione reale, non sempre questo è possibile, proprio perchè possono mancare risorse umane competenti o tempistiche adeguate.

        Esistono differenti algoritmi per l'apprendimento automatico mediante un sistema semi-supervisionato:
        * Self training
        * Multi-view training
        * Self-ensembling 

        Per quanto riguarda il multi-view training, esso mira a formare diversi modelli con diverse visualizzazioni dei dati. Idealmente, queste viste sono complementari e i modelli possono collaborare per migliorare il risultato finale. Queste viste possono differire in diversi modi, ad esempio nelle funzionalità che utilizzano, nelle architetture dei modelli o nei dati su cui i modelli vengono formati.

        Il self-ensembling, come il multi-view training, punta a combinare diverse varianti dei modelli. A differenza però di quest'ultimo, la diversità nei modelli non è un punto chiave perchè il self-ensembling utilizza principalmente un singolo modello in diverse configurazioni al fine di rendere più affidabili le previsioni del modello. 

        A titolo di esempio vedremo più in dettaglio l'algoritmo di "Auto-allenamento" che è stato uno dei primi ad essere sviluppato ed è l'esempio più diretto di come le previsioni di un modello possono essere incorporate nel training del modello.

        L'algoritmo di auto-allenamento prevede quindi di basarsi per quanto può su dei dati che sono stati preventivamente definiti secondo un particolare modo e altri che sono solamente dei dati da studiare e analizzare. Questi ultimi vengono comuqnue utilizzati, ma in maniera più cauta, ovvero, prima di allenare il modello per imparare a fare una predizione sui futuri elementi dati in input, il modello si concentrerà ad etichettare gli input che ancora sono senza alcuna etichetta.

        Come viene spiegato in un articolo su ruder.io (https://ruder.io/semi-supervised/index.html#relatedmethodsandareas), la logica di classificazione dei dati non ancora classificati segue quanto scritto:
        "Formalmente, l'auto etichettamento avviene su un modello M avente un training set etichetto L con delle etichette contenute in C e un set non etichettato U. Ad ogni iterazione, per ogni x in U, il modello fornisce delle predizioni su m(x) sottoforma di probabilità p(x, c) ovvero probabilità che x appartenga alla classe c per ogni c in C. Tra le probabilità appena calcolate, definiamo P(x, c) come la probabilità avente il valore maggiore, allora se P è maggiore di una soglia T, x verrà aggiunto a L con l'etichetta c. Questo processo viene ripetuto per un numero fisso di iterazioni o fino a quando non ci sono più dati da etichettare.".[nota: traduzione libera del testo: nell'originale è presente una formula ceh io rendo a parole]

        Di seguito vediamo uno pseduo-codice che segue quanto detto sopra:

        \usepackage{algorithm}
        \usepackage[noend]{algpseudocode}
        \usepackage{amsmath}
        \usepackage{amsfonts}
        \begin{algorithmic}[1]
            \Repeat
                \State $m \gets train\_model(L)$
                \For {$x \in U$}
                    \If {$\max m(x) > \tau$}
                        \State $L \gets L \cup \{(x, p(x))\}$
                    \EndIf
                \EndFor
            \Until {no more predictions are confident}
        \end{algorithmic}
        
        Sezione 1.1.4: Reinforcement learning

        Il quarto ed ultimo approccio chiamato Reinforcement learning, è un approccio che si differenzia da quelli visti fino ad ora. Questo paradigma si occupa di problemi di decisioni sequenziali, in cui l'azione da compiere dipende dallo stato attuale del sistema e ne determina quello futuro. In altre parole, questo è un sistema dinamico che può apprendere in seguito ad ogni decisione presa, a prescindere che questa sia giusta o sbagliata.

        [IMMAGINE DELLO SCENARIO DI RL] (https://www.aitrends.com/education/udacitys-school-of-ai-opens-the-new-deep-reinforcement-learning-nanodegree-program-for-enrollment/) (https://www.guru99.com/reinforcement-learning-tutorial.html)

        Quando il sistema prende una decisione, esso otterrà una "ricompensa" che può essere un punteggio, che sarà alto o basso a seconda se la decisione presa è giusta o sbagliata. Con questa logica, la macchina cercherà di fare sempre meglio per arrivare a ottenere il punteggio più alto possibile, prendendo così solo le decisioni corrette. 

        Di seguito, sono citati dei casi di utilizzo di questo approccio:

        * Imparare a giocare a scacchi
        * Imparare a guidare un veicolo

    Sezione 1.2: Ridimensionamento delle feature

    Nel machine learning, come abbiamo visto, l'input, gioca un ruolo fondamentale nello sviluppo di un modello, che riesca a predire nel modo corretto i nuovi dati che verrano esaminati da quest'ultimo.
    In questo testo ci concentreremo proprio sul riconoscimento dei volti all'interno delle immagini, quindi per per parlare dell'importanza del ridimensionamento delle feature, faremo riferimento proprio all'analisi di immagini che raffigurano persone, animali o oggetti.

    Spesso, ci troviamo davanti a dei dati di dimensioni eccessive, i quali comportano in primis dei problemi a livello tempistico e anche a problemi a livello computativo.
    
    Ci basti pensare che quando si cerca di analizzare un'immagine per estrapolarne delle informazioni (riconoscimento di oggetti, persone, animali) dobbiamo prendere passare in rassegna tutti i pixel! Supponiamo di prendere anche un'immagine, a bassa risoluzione, ad esempio 500x500, significherebbe trovarsi davanti a 500**2 pixel, ovvero 250.000 elementi per una singola immagine! Questo comporterebbe quindi, di analizzare uno spazio sovra dimensionato, con, appunto 250.000 dimensioni.

    Pensare ad uno spazio di quelle dimensioni è impensabile, proprio per questo ci vengono in soccorso delle tecniche che si occupano di ridurre il numero di elementi che definsicono l'oggetto, senza perdere, o meglio, estrapolando, solamente le informazioni più utili che permettono di differenziare un oggetto da un'altro.

    Pensiamo ad esempio a un'immagine in cui è raffigurato il volto di una persona. E' normale pensare che non tutti i pixel siano di fondamentale importanza per riconoscere il soggetto raffigurato. Ad esempio, tutti i pixel presenti nei bordi dell'immagine saranno sicuramente da scartare in quanto non ci diranno niente sulla persona raffigurata, così come molti altri pixel che raffigurano parti poco interessanti, ad esempio lo sfondo dell'immagine. Vediamo nel dettaglio quali sono gli strumenti più utilizzati per risolvere questo tipo di problemi.

    *** Sezione 1.2.1: PCA (Principal Component Analysis)

    PCA (Principale Component Analysis) è un metodo di riduzione delle dimensionalità di un oggetto. Lo scopo di PCA è quindi quello di diminuire il numero di variabili, limitando il più possibile la perdita di informazioni. 
    In primo luogo, viene calcolata la media per ogni feature. Una volta calcolata, il vettore risultante lo faremo coincidere con l'origine degli assi, in questo modo ogni punto verrà traslato di conseguenza. A questo punto viene calcolata la retta che meglio si adatta a tutti i punti, ovvero la retta R che minimizza la somma delle distanze dei punti da R. Questa viene chiamata principal component 1. Si ripete questo passaggio per ogni dimensione, mantendo la perpendicolarità della nuova retta (o principal component_{n}) rispetto all'ultima retta calcolata (o principal component_{n-1}). 
    Una volta calcolate tutte le principal component dello spazio PCA si va a calcolare, per ognuna di esse, l'autovettore, il quale sarò utilizzato per determinare la nuova posizione di ogni punto sul rispettivo asse. Una volta scalati tutti i punti si può calcolare la varianza per ogni asse. Il valore della varianza x, calcolata in percentuale, ci dice quanto pesa l'inforamzione contenuta sull'asse x, questo ci permetterà quindi, di eliminare gli assi meno interessanti, ovvero gli assi con la varianza più bassa. 


    Sezione 1.2: Utilizzi

    Machine Learning: le applicazioni
    Le applicazioni di Machine Learning sono già oggi molto numerose, alcune delle quali entrate comunemente nella nostra vita quotidiana senza che in realtà ce ne rendessimo conto.

    Pensiamo per esempio all’utilizzo dei motori di ricerca: attraverso una o più parole chiave, questi motori restituiscono liste di risultati (le cosiddette SERP – Search Engine Results Page) che sono l’effetto di algoritmi di Machine Learning con apprendimento non supervisionato (forniscono come output informazioni ritenute attinenti alla ricerca effettuata in base all’analisi di schemi, modelli, strutture nei dati).

    Altro esempio comune è legato ai filtri anti-spam delle e-mail basati su sistemi di Machine Learning che imparano continuamente sia ad intercettare messaggi di posta elettronica sospetti o fraudolenti sia ad agire di conseguenza (per esempio eliminandoli prima che vengano distribuiti sulle caselle personali degli utenti). Sistemi di questo tipo, anche con sofisticazioni maggiori, vengono per esempio impiegati anche nel settore Finance per la prevenzione delle frodi (come la clonazione della carta di credito), dei furti di dati e identità; gli algoritmi imparano ad agire mettendo in correlazione eventi, abitudini degli utenti, preferenze di spesa, ecc.; informazioni attraverso le quali riescono poi a identificare in real-time eventuali comportamenti anomali che potrebbero appunto identificare un furto od una frode.

    Interessanti esempi di Machine Learning con apprendimento supervisionato arrivano dal settore della ricerca scientifica in campo medico dove gli algoritmi imparano a fare previsioni sempre più accurate per prevenire lo scatenarsi di epidemie oppure per effettuare diagnosi di tumori o malattie rare in modo accurato e tempestivo.

    E ancora, sempre nell’ambito dell’apprendimento supervisionato, ci sono interessanti applicazioni di Machine Learning a livello di riconoscimento vocale o identificazione della scrittura manuale.

    Come accennato, i sistemi che si basano sull’apprendimento con rinforzo stanno alla base dello sviluppo delle auto a guida autonoma che, proprio attraverso il Machine Learning, imparano a riconoscere l’ambiente circostante (con i dati raccolti da sensori, GPS, ecc.) e ad adattare il loro “comportamento” in base alle specifiche situazioni che devono affrontare/superare.

    Anche i cosiddetti sistemi di raccomandazione sfruttano il Machine Learning imparando dal comportamento e dalle preferenze degli utenti che navigano su siti web, piattaforme o applicazioni mobile; ne sono un esempio quelli che comunemente ci siamo abituati a vedere ed utilizzare sulle piattaforme di eCommerce come Amazon o di intrattenimento e accesso a contenuti come Netflix o Spotify.

/*******************************************************************************************************************************************************************************************************/
